{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Quickstart","text":"<p>Adala is an Autonomous DAta (Labeling) Agent framework.</p> <p>Adala offers a robust framework for implementing agents specialized in data processing, with an emphasis on diverse data labeling tasks. These agents are autonomous, meaning they can independently acquire one or more skills through iterative learning. This learning process is influenced by their operating environment, observations, and reflections. Users define the environment by providing a ground truth dataset. Every agent learns and applies its skills in what we refer to as a \"runtime\", synonymous with LLM.</p> <p></p>"},{"location":"#installation","title":"Installation","text":"<p>Install Adala:</p> <pre><code>pip install adala\n</code></pre>"},{"location":"#prerequisites","title":"Prerequisites","text":"<p>Set OPENAI_API_KEY (see instructions here)</p>"},{"location":"#quickstart_1","title":"\ud83c\udfac Quickstart","text":"<p>In this example we will use Adala as a standalone library directly inside Python notebook.</p> <p>Click here to see an extended quickstart example. </p> <pre><code>import pandas as pd\n\nfrom adala.agents import Agent\nfrom adala.environments import StaticEnvironment\nfrom adala.skills import ClassificationSkill\nfrom adala.runtimes import OpenAIChatRuntime\nfrom rich import print\n\n# Train dataset\ntrain_df = pd.DataFrame([\n    [\"It was the negative first impressions, and then it started working.\", \"Positive\"],\n    [\"Not loud enough and doesn't turn on like it should.\", \"Negative\"],\n    [\"I don't know what to say.\", \"Neutral\"],\n    [\"Manager was rude, but the most important that mic shows very flat frequency response.\", \"Positive\"],\n    [\"The phone doesn't seem to accept anything except CBR mp3s.\", \"Negative\"],\n    [\"I tried it before, I bought this device for my son.\", \"Neutral\"],\n], columns=[\"text\", \"sentiment\"])\n\n# Test dataset\ntest_df = pd.DataFrame([\n    \"All three broke within two months of use.\",\n    \"The device worked for a long time, can't say anything bad.\",\n    \"Just a random line of text.\"\n], columns=[\"text\"])\n\nagent = Agent(\n    # connect to a dataset\n    environment=StaticEnvironment(df=train_df),\n\n    # define a skill\n    skills=ClassificationSkill(\n        name='sentiment',\n        instructions=\"Label text as positive, negative or neutral.\",\n        labels={'sentiment': [\"Positive\", \"Negative\", \"Neutral\"]},\n        input_template=\"Text: {text}\",\n        output_template=\"Sentiment: {sentiment}\"\n    ),\n\n    # define all the different runtimes your skills may use\n    runtimes = {\n        # You can specify your OPENAI API KEY here via `OpenAIRuntime(..., api_key='your-api-key')`\n        'openai': OpenAIChatRuntime(model='gpt-3.5-turbo'),\n    },\n    default_runtime='openai',\n\n    # NOTE! If you have access to GPT-4, you can uncomment the lines bellow for better results\n#     default_teacher_runtime='openai-gpt4',\n#     teacher_runtimes = {\n#       'openai-gpt4': OpenAIRuntime(model='gpt-4')\n#     }\n)\n\nprint(agent)\nprint(agent.skills)\n\nagent.learn(learning_iterations=3, accuracy_threshold=0.95)\n\nprint('\\n=&gt; Run tests ...')\npredictions = agent.run(test_df)\nprint('\\n =&gt; Test results:')\nprint(predictions)\n</code></pre>"},{"location":"#reference","title":"Reference","text":"<ul> <li>Agents - main interface for  interacting with environment</li> <li>Datasets - data inputs for agents</li> <li>Environments - environments for agents, where it collects ground truth signal</li> <li>Memories - agent's memory for storing and retrieving data</li> <li>Runtimes - agent's execution runtime (e.g. LLMs providers)</li> <li>Skills - agent skills for data labeling</li> </ul>"},{"location":"agents/","title":"Agents","text":""},{"location":"agents/#adala.agents.base.Agent","title":"<code>Agent</code>","text":"<p>             Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Represents a customizable agent that can interact with environments, employ skills, and leverage memory and runtimes.</p> <p>Attributes:</p> Name Type Description <code>environment</code> <code>Environment</code> <p>The environment with which the agent interacts.</p> <code>skills</code> <code>Union[SkillSet, List[Skill]]</code> <p>The skills possessed by the agent.</p> <code>memory</code> <code>LongTermMemory</code> <p>The agent's long-term memory. Defaults to None.</p> <code>runtimes</code> <code>Dict[str, Runtime]</code> <p>The runtimes available to the agent. Defaults to predefined runtimes.</p> <code>default_runtime</code> <code>str</code> <p>The default runtime used by the agent. Defaults to 'openai'.</p> <code>teacher_runtimes</code> <code>Dict[str, Runtime]</code> <p>The runtimes available to the agent's teacher. Defaults to predefined runtimes.</p> <code>default_teacher_runtime</code> <code>str</code> <p>The default runtime used by the agent's teacher. Defaults to 'openai-gpt3'.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from adala.environments import StaticEnvironment\n&gt;&gt;&gt; from adala.skills import LinearSkillSet, TransformSkill\n&gt;&gt;&gt; from adala.agents import Agent\n&gt;&gt;&gt; agent = Agent(skills=LinearSkillSet(skills=[TransformSkill()]), environment=StaticEnvironment())\n&gt;&gt;&gt; agent.learn()  # starts the learning process\n&gt;&gt;&gt; predictions = agent.run()  # runs the agent and returns the predictions\n</code></pre> Source code in <code>adala/agents/base.py</code> <pre><code>class Agent(BaseModel, ABC):\n    \"\"\"\n    Represents a customizable agent that can interact with environments,\n    employ skills, and leverage memory and runtimes.\n\n    Attributes:\n        environment (Environment): The environment with which the agent interacts.\n        skills (Union[SkillSet, List[Skill]]): The skills possessed by the agent.\n        memory (LongTermMemory, optional): The agent's long-term memory. Defaults to None.\n        runtimes (Dict[str, Runtime], optional): The runtimes available to the agent. Defaults to predefined runtimes.\n        default_runtime (str): The default runtime used by the agent. Defaults to 'openai'.\n        teacher_runtimes (Dict[str, Runtime], optional): The runtimes available to the agent's teacher. Defaults to predefined runtimes.\n        default_teacher_runtime (str): The default runtime used by the agent's teacher. Defaults to 'openai-gpt3'.\n\n    Examples:\n        &gt;&gt;&gt; from adala.environments import StaticEnvironment\n        &gt;&gt;&gt; from adala.skills import LinearSkillSet, TransformSkill\n        &gt;&gt;&gt; from adala.agents import Agent\n        &gt;&gt;&gt; agent = Agent(skills=LinearSkillSet(skills=[TransformSkill()]), environment=StaticEnvironment())\n        &gt;&gt;&gt; agent.learn()  # starts the learning process\n        &gt;&gt;&gt; predictions = agent.run()  # runs the agent and returns the predictions\n\n    \"\"\"\n\n    environment: Optional[Environment] = None\n    skills: SkillSet\n\n    memory: Memory = Field(default=None)\n    runtimes: Dict[str, Runtime] = Field(\n        default_factory=lambda: {\n            \"openai\": GuidanceRuntime()\n            # 'openai': OpenAIChatRuntime(model='gpt-3.5-turbo'),\n            # 'llama2': LLMRuntime(\n            #     llm_runtime_type=LLMRuntimeModelType.Transformers,\n            #     llm_params={\n            #         'model': 'meta-llama/Llama-2-7b',\n            #         'device': 'cuda:0',\n            #     }\n            # )\n        }\n    )\n    teacher_runtimes: Dict[str, Runtime] = Field(\n        default_factory=lambda: {\n            \"openai-gpt3\": OpenAIChatRuntime(model=\"gpt-3.5-turbo\"),\n            # 'openai-gpt4': OpenAIChatRuntime(model='gpt-4')\n        }\n    )\n    default_runtime: str = \"openai\"\n    default_teacher_runtime: str = \"openai-gpt3\"\n\n    class Config:\n        arbitrary_types_allowed = True\n\n    def __rich__(self) -&gt; str:\n        \"\"\"\n        Returns a colorized and formatted representation of the Agent instance.\n\n        Returns:\n            str: A rich-formatted representation of the agent.\n        \"\"\"\n\n        skill_names = \", \".join([skill.name for skill in self.skills.skills.values()])\n        runtime_names = \", \".join(self.runtimes.keys())\n\n        return (\n            f\"[bold blue]Agent Instance[/bold blue]\\n\\n\"\n            f\"Environment: {self.environment.__class__.__name__}\\n\"\n            f\"Skills: {skill_names}\\n\"\n            f\"Runtimes: {runtime_names}\\n\"\n            f\"Default Runtime: {self.default_runtime}\\n\"\n            f\"Default Teacher Runtime: {self.default_teacher_runtime}\"\n        )\n\n    @field_validator(\"environment\", mode=\"before\")\n    def environment_validator(cls, v) -&gt; Environment:\n        \"\"\"\n        Validates and possibly transforms the environment attribute:\n        if the environment is an InternalDataFrame, it is transformed into a StaticEnvironment.\n        \"\"\"\n        if isinstance(v, InternalDataFrame):\n            v = StaticEnvironment(df=v)\n        return v\n\n    @field_validator(\"skills\", mode=\"before\")\n    def skills_validator(cls, v) -&gt; SkillSet:\n        \"\"\"\n        Validates and possibly transforms the skills attribute.\n        \"\"\"\n        if isinstance(v, SkillSet):\n            return v\n        elif isinstance(v, Skill):\n            return LinearSkillSet(skills=[v])\n        else:\n            raise ValueError(f\"skills must be of type SkillSet or Skill, not {type(v)}\")\n\n    @model_validator(mode=\"after\")\n    def verify_input_parameters(self):\n        \"\"\"\n        Verifies that the input parameters are valid.\"\"\"\n\n        def _raise_default_runtime_error(val, runtime, runtimes, default_value):\n            print_error(\n                f\"The Agent.{runtime} is set to {val}, \"\n                f\"but this runtime is not available in the list: {list(runtimes)}. \"\n                f\"Please choose one of the available runtimes and initialize the agent again, for example:\\n\\n\"\n                f\"agent = Agent(..., {runtime}='{default_value}')\\n\\n\"\n                f\"Make sure the default runtime is available in the list of runtimes. For example:\\n\\n\"\n                f\"agent = Agent(..., runtimes={{'{default_value}': OpenAIRuntime(model='gpt-4')}})\\n\\n\"\n            )\n            raise ValueError(f\"default runtime {val} not found in provided runtimes.\")\n\n        if self.default_runtime not in self.runtimes:\n            _raise_default_runtime_error(\n                self.default_runtime, \"default_runtime\", self.runtimes, \"openai\"\n            )\n        if self.default_teacher_runtime not in self.teacher_runtimes:\n            _raise_default_runtime_error(\n                self.default_teacher_runtime,\n                \"default_teacher_runtime\",\n                self.teacher_runtimes,\n                \"openai-gpt4\",\n            )\n        return self\n\n    def get_runtime(self, runtime: Optional[str] = None) -&gt; Runtime:\n        \"\"\"\n        Retrieves the specified runtime or the default runtime if none is specified.\n\n        Args:\n            runtime (str, optional): The name of the runtime to retrieve. Defaults to None.\n\n        Returns:\n            Runtime: The requested runtime.\n\n        Raises:\n            ValueError: If the specified runtime is not found.\n        \"\"\"\n\n        if runtime is None:\n            runtime = self.default_runtime\n        if runtime not in self.runtimes:\n            raise ValueError(f'Runtime \"{runtime}\" not found.')\n        return self.runtimes[runtime]\n\n    def get_teacher_runtime(self, runtime: Optional[str] = None) -&gt; Runtime:\n        \"\"\"\n        Retrieves the specified teacher runtime or the default runtime if none is specified.\n\n        Args:\n            runtime (str, optional): The name of the runtime to retrieve. Defaults to None.\n\n        Returns:\n            Runtime: The requested runtime.\n\n        Raises:\n            ValueError: If the specified runtime is not found.\n        \"\"\"\n\n        if runtime is None:\n            runtime = self.default_teacher_runtime\n        if runtime not in self.teacher_runtimes:\n            raise ValueError(f'Teacher Runtime \"{runtime}\" not found.')\n        return self.teacher_runtimes[runtime]\n\n    def run(\n        self, input: InternalDataFrame = None, runtime: Optional[str] = None\n    ) -&gt; InternalDataFrame:\n        \"\"\"\n        Runs the agent on the specified dataset.\n\n        Args:\n            input (InternalDataFrame): The dataset to run the agent on.\n            runtime (str, optional): The name of the runtime to use. Defaults to None, use the default runtime.\n\n        Returns:\n            InternalDataFrame: The dataset with the agent's predictions.\n        \"\"\"\n        if input is None:\n            if self.environment is None:\n                raise ValueError(\"input is None and no environment is set.\")\n            input = self.environment.get_data_batch()\n        runtime = self.get_runtime(runtime=runtime)\n        predictions = self.skills.apply(input, runtime=runtime)\n        return predictions\n\n    def select_skill_to_train(\n        self, feedback: EnvironmentFeedback, accuracy_threshold: float\n    ) -&gt; Tuple[str, str, float]:\n        \"\"\"\n        Selects the skill to train based on the feedback signal.\n\n        Args:\n            feedback (Feedback): The feedback signal.\n            accuracy_threshold (float): The accuracy threshold to use for selecting the skill to train.\n\n        Returns:\n            str: The name of the skill to train.\n            str: The name of the skill output to train.\n            float: The accuracy score of the skill to train.\n\n        \"\"\"\n\n        # Use ground truth signal to find the skill to improve\n        # TODO: what if it is not possible to estimate accuracy per skill?\n        accuracy = feedback.get_accuracy()\n        train_skill_name, train_skill_output, acc_score = \"\", \"\", None\n        for skill_output, skill_name in self.skills.get_skill_outputs().items():\n            if skill_output in accuracy and accuracy[skill_output] &lt; accuracy_threshold:\n                train_skill_name, train_skill_output = skill_name, skill_output\n                acc_score = accuracy[skill_output]\n                break\n\n        return train_skill_name, train_skill_output, acc_score\n\n    def learn(\n        self,\n        learning_iterations: int = 3,\n        accuracy_threshold: float = 0.9,\n        update_memory: bool = True,\n        batch_size: Optional[int] = None,\n        num_feedbacks: Optional[int] = None,\n        runtime: Optional[str] = None,\n        teacher_runtime: Optional[str] = None,\n    ):\n        \"\"\"\n        Enables the agent to learn and improve its skills based on interactions with its environment.\n\n        Args:\n            learning_iterations (int, optional): The number of iterations for learning. Defaults to 3.\n            accuracy_threshold (float, optional): The desired accuracy threshold to reach. Defaults to 0.9.\n            update_memory (bool, optional): Flag to determine if memory should be updated after learning. Defaults to True.\n            num_feedbacks (int, optional): The number of predictions to request feedback for. Defaults to None.\n            runtime (str, optional): The runtime to be used for the learning process. Defaults to None.\n            teacher_runtime (str, optional): The teacher runtime to be used for the learning process. Defaults to None.\n        \"\"\"\n\n        runtime = self.get_runtime(runtime=runtime)\n        teacher_runtime = self.get_teacher_runtime(runtime=teacher_runtime)\n\n        for iteration in range(learning_iterations):\n            print_text(\n                f\"\\n\\n=&gt; Iteration #{iteration}: Getting feedback, analyzing and improving ...\"\n            )\n\n            inputs = self.environment.get_data_batch(batch_size=batch_size)\n            predictions = self.skills.apply(inputs, runtime=runtime)\n            feedback = self.environment.get_feedback(\n                self.skills, predictions, num_feedbacks=num_feedbacks\n            )\n            # TODO: this is just pretty printing - remove later for efficiency\n            print(\"Predictions and feedback:\")\n            print_dataframe(\n                feedback.feedback.rename(\n                    columns=lambda x: x + \"__fb\" if x in predictions.columns else x\n                ).merge(predictions, left_index=True, right_index=True)\n            )\n            # -----------------------------\n            skill_mismatch = feedback.match.fillna(True) == False\n            has_errors = skill_mismatch.any(axis=1).any()\n            if not has_errors:\n                print_text(\"No errors found!\")\n                continue\n            first_skill_with_errors = skill_mismatch.any(axis=0).idxmax()\n\n            accuracy = feedback.get_accuracy()\n            # TODO: iterating over skill can be more complex, and we should take order into account\n            for skill_output, skill_name in self.skills.get_skill_outputs().items():\n                skill = self.skills[skill_name]\n                if skill.frozen:\n                    continue\n\n                print_text(\n                    f'Skill output to improve: \"{skill_output}\" (Skill=\"{skill_name}\")\\n'\n                    f\"Accuracy = {accuracy[skill_output] * 100:0.2f}%\",\n                    style=\"bold red\",\n                )\n\n                old_instructions = skill.instructions\n                skill.improve(\n                    predictions, skill_output, feedback, runtime=teacher_runtime\n                )\n\n                if is_running_in_jupyter():\n                    highlight_differences(old_instructions, skill.instructions)\n                else:\n                    print_text(skill.instructions, style=\"bold green\")\n\n                if skill_name == first_skill_with_errors:\n                    break\n\n        print_text(\"Train is done!\")\n</code></pre>"},{"location":"agents/#adala.agents.base.Agent.__rich__","title":"<code>__rich__()</code>","text":"<p>Returns a colorized and formatted representation of the Agent instance.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A rich-formatted representation of the agent.</p> Source code in <code>adala/agents/base.py</code> <pre><code>def __rich__(self) -&gt; str:\n    \"\"\"\n    Returns a colorized and formatted representation of the Agent instance.\n\n    Returns:\n        str: A rich-formatted representation of the agent.\n    \"\"\"\n\n    skill_names = \", \".join([skill.name for skill in self.skills.skills.values()])\n    runtime_names = \", \".join(self.runtimes.keys())\n\n    return (\n        f\"[bold blue]Agent Instance[/bold blue]\\n\\n\"\n        f\"Environment: {self.environment.__class__.__name__}\\n\"\n        f\"Skills: {skill_names}\\n\"\n        f\"Runtimes: {runtime_names}\\n\"\n        f\"Default Runtime: {self.default_runtime}\\n\"\n        f\"Default Teacher Runtime: {self.default_teacher_runtime}\"\n    )\n</code></pre>"},{"location":"agents/#adala.agents.base.Agent.environment_validator","title":"<code>environment_validator(v)</code>","text":"<p>Validates and possibly transforms the environment attribute: if the environment is an InternalDataFrame, it is transformed into a StaticEnvironment.</p> Source code in <code>adala/agents/base.py</code> <pre><code>@field_validator(\"environment\", mode=\"before\")\ndef environment_validator(cls, v) -&gt; Environment:\n    \"\"\"\n    Validates and possibly transforms the environment attribute:\n    if the environment is an InternalDataFrame, it is transformed into a StaticEnvironment.\n    \"\"\"\n    if isinstance(v, InternalDataFrame):\n        v = StaticEnvironment(df=v)\n    return v\n</code></pre>"},{"location":"agents/#adala.agents.base.Agent.get_runtime","title":"<code>get_runtime(runtime=None)</code>","text":"<p>Retrieves the specified runtime or the default runtime if none is specified.</p> <p>Parameters:</p> Name Type Description Default <code>runtime</code> <code>str</code> <p>The name of the runtime to retrieve. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Runtime</code> <code>Runtime</code> <p>The requested runtime.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified runtime is not found.</p> Source code in <code>adala/agents/base.py</code> <pre><code>def get_runtime(self, runtime: Optional[str] = None) -&gt; Runtime:\n    \"\"\"\n    Retrieves the specified runtime or the default runtime if none is specified.\n\n    Args:\n        runtime (str, optional): The name of the runtime to retrieve. Defaults to None.\n\n    Returns:\n        Runtime: The requested runtime.\n\n    Raises:\n        ValueError: If the specified runtime is not found.\n    \"\"\"\n\n    if runtime is None:\n        runtime = self.default_runtime\n    if runtime not in self.runtimes:\n        raise ValueError(f'Runtime \"{runtime}\" not found.')\n    return self.runtimes[runtime]\n</code></pre>"},{"location":"agents/#adala.agents.base.Agent.get_teacher_runtime","title":"<code>get_teacher_runtime(runtime=None)</code>","text":"<p>Retrieves the specified teacher runtime or the default runtime if none is specified.</p> <p>Parameters:</p> Name Type Description Default <code>runtime</code> <code>str</code> <p>The name of the runtime to retrieve. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Runtime</code> <code>Runtime</code> <p>The requested runtime.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified runtime is not found.</p> Source code in <code>adala/agents/base.py</code> <pre><code>def get_teacher_runtime(self, runtime: Optional[str] = None) -&gt; Runtime:\n    \"\"\"\n    Retrieves the specified teacher runtime or the default runtime if none is specified.\n\n    Args:\n        runtime (str, optional): The name of the runtime to retrieve. Defaults to None.\n\n    Returns:\n        Runtime: The requested runtime.\n\n    Raises:\n        ValueError: If the specified runtime is not found.\n    \"\"\"\n\n    if runtime is None:\n        runtime = self.default_teacher_runtime\n    if runtime not in self.teacher_runtimes:\n        raise ValueError(f'Teacher Runtime \"{runtime}\" not found.')\n    return self.teacher_runtimes[runtime]\n</code></pre>"},{"location":"agents/#adala.agents.base.Agent.learn","title":"<code>learn(learning_iterations=3, accuracy_threshold=0.9, update_memory=True, batch_size=None, num_feedbacks=None, runtime=None, teacher_runtime=None)</code>","text":"<p>Enables the agent to learn and improve its skills based on interactions with its environment.</p> <p>Parameters:</p> Name Type Description Default <code>learning_iterations</code> <code>int</code> <p>The number of iterations for learning. Defaults to 3.</p> <code>3</code> <code>accuracy_threshold</code> <code>float</code> <p>The desired accuracy threshold to reach. Defaults to 0.9.</p> <code>0.9</code> <code>update_memory</code> <code>bool</code> <p>Flag to determine if memory should be updated after learning. Defaults to True.</p> <code>True</code> <code>num_feedbacks</code> <code>int</code> <p>The number of predictions to request feedback for. Defaults to None.</p> <code>None</code> <code>runtime</code> <code>str</code> <p>The runtime to be used for the learning process. Defaults to None.</p> <code>None</code> <code>teacher_runtime</code> <code>str</code> <p>The teacher runtime to be used for the learning process. Defaults to None.</p> <code>None</code> Source code in <code>adala/agents/base.py</code> <pre><code>def learn(\n    self,\n    learning_iterations: int = 3,\n    accuracy_threshold: float = 0.9,\n    update_memory: bool = True,\n    batch_size: Optional[int] = None,\n    num_feedbacks: Optional[int] = None,\n    runtime: Optional[str] = None,\n    teacher_runtime: Optional[str] = None,\n):\n    \"\"\"\n    Enables the agent to learn and improve its skills based on interactions with its environment.\n\n    Args:\n        learning_iterations (int, optional): The number of iterations for learning. Defaults to 3.\n        accuracy_threshold (float, optional): The desired accuracy threshold to reach. Defaults to 0.9.\n        update_memory (bool, optional): Flag to determine if memory should be updated after learning. Defaults to True.\n        num_feedbacks (int, optional): The number of predictions to request feedback for. Defaults to None.\n        runtime (str, optional): The runtime to be used for the learning process. Defaults to None.\n        teacher_runtime (str, optional): The teacher runtime to be used for the learning process. Defaults to None.\n    \"\"\"\n\n    runtime = self.get_runtime(runtime=runtime)\n    teacher_runtime = self.get_teacher_runtime(runtime=teacher_runtime)\n\n    for iteration in range(learning_iterations):\n        print_text(\n            f\"\\n\\n=&gt; Iteration #{iteration}: Getting feedback, analyzing and improving ...\"\n        )\n\n        inputs = self.environment.get_data_batch(batch_size=batch_size)\n        predictions = self.skills.apply(inputs, runtime=runtime)\n        feedback = self.environment.get_feedback(\n            self.skills, predictions, num_feedbacks=num_feedbacks\n        )\n        # TODO: this is just pretty printing - remove later for efficiency\n        print(\"Predictions and feedback:\")\n        print_dataframe(\n            feedback.feedback.rename(\n                columns=lambda x: x + \"__fb\" if x in predictions.columns else x\n            ).merge(predictions, left_index=True, right_index=True)\n        )\n        # -----------------------------\n        skill_mismatch = feedback.match.fillna(True) == False\n        has_errors = skill_mismatch.any(axis=1).any()\n        if not has_errors:\n            print_text(\"No errors found!\")\n            continue\n        first_skill_with_errors = skill_mismatch.any(axis=0).idxmax()\n\n        accuracy = feedback.get_accuracy()\n        # TODO: iterating over skill can be more complex, and we should take order into account\n        for skill_output, skill_name in self.skills.get_skill_outputs().items():\n            skill = self.skills[skill_name]\n            if skill.frozen:\n                continue\n\n            print_text(\n                f'Skill output to improve: \"{skill_output}\" (Skill=\"{skill_name}\")\\n'\n                f\"Accuracy = {accuracy[skill_output] * 100:0.2f}%\",\n                style=\"bold red\",\n            )\n\n            old_instructions = skill.instructions\n            skill.improve(\n                predictions, skill_output, feedback, runtime=teacher_runtime\n            )\n\n            if is_running_in_jupyter():\n                highlight_differences(old_instructions, skill.instructions)\n            else:\n                print_text(skill.instructions, style=\"bold green\")\n\n            if skill_name == first_skill_with_errors:\n                break\n\n    print_text(\"Train is done!\")\n</code></pre>"},{"location":"agents/#adala.agents.base.Agent.run","title":"<code>run(input=None, runtime=None)</code>","text":"<p>Runs the agent on the specified dataset.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>InternalDataFrame</code> <p>The dataset to run the agent on.</p> <code>None</code> <code>runtime</code> <code>str</code> <p>The name of the runtime to use. Defaults to None, use the default runtime.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>InternalDataFrame</code> <code>InternalDataFrame</code> <p>The dataset with the agent's predictions.</p> Source code in <code>adala/agents/base.py</code> <pre><code>def run(\n    self, input: InternalDataFrame = None, runtime: Optional[str] = None\n) -&gt; InternalDataFrame:\n    \"\"\"\n    Runs the agent on the specified dataset.\n\n    Args:\n        input (InternalDataFrame): The dataset to run the agent on.\n        runtime (str, optional): The name of the runtime to use. Defaults to None, use the default runtime.\n\n    Returns:\n        InternalDataFrame: The dataset with the agent's predictions.\n    \"\"\"\n    if input is None:\n        if self.environment is None:\n            raise ValueError(\"input is None and no environment is set.\")\n        input = self.environment.get_data_batch()\n    runtime = self.get_runtime(runtime=runtime)\n    predictions = self.skills.apply(input, runtime=runtime)\n    return predictions\n</code></pre>"},{"location":"agents/#adala.agents.base.Agent.select_skill_to_train","title":"<code>select_skill_to_train(feedback, accuracy_threshold)</code>","text":"<p>Selects the skill to train based on the feedback signal.</p> <p>Parameters:</p> Name Type Description Default <code>feedback</code> <code>Feedback</code> <p>The feedback signal.</p> required <code>accuracy_threshold</code> <code>float</code> <p>The accuracy threshold to use for selecting the skill to train.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The name of the skill to train.</p> <code>str</code> <code>str</code> <p>The name of the skill output to train.</p> <code>float</code> <code>float</code> <p>The accuracy score of the skill to train.</p> Source code in <code>adala/agents/base.py</code> <pre><code>def select_skill_to_train(\n    self, feedback: EnvironmentFeedback, accuracy_threshold: float\n) -&gt; Tuple[str, str, float]:\n    \"\"\"\n    Selects the skill to train based on the feedback signal.\n\n    Args:\n        feedback (Feedback): The feedback signal.\n        accuracy_threshold (float): The accuracy threshold to use for selecting the skill to train.\n\n    Returns:\n        str: The name of the skill to train.\n        str: The name of the skill output to train.\n        float: The accuracy score of the skill to train.\n\n    \"\"\"\n\n    # Use ground truth signal to find the skill to improve\n    # TODO: what if it is not possible to estimate accuracy per skill?\n    accuracy = feedback.get_accuracy()\n    train_skill_name, train_skill_output, acc_score = \"\", \"\", None\n    for skill_output, skill_name in self.skills.get_skill_outputs().items():\n        if skill_output in accuracy and accuracy[skill_output] &lt; accuracy_threshold:\n            train_skill_name, train_skill_output = skill_name, skill_output\n            acc_score = accuracy[skill_output]\n            break\n\n    return train_skill_name, train_skill_output, acc_score\n</code></pre>"},{"location":"agents/#adala.agents.base.Agent.skills_validator","title":"<code>skills_validator(v)</code>","text":"<p>Validates and possibly transforms the skills attribute.</p> Source code in <code>adala/agents/base.py</code> <pre><code>@field_validator(\"skills\", mode=\"before\")\ndef skills_validator(cls, v) -&gt; SkillSet:\n    \"\"\"\n    Validates and possibly transforms the skills attribute.\n    \"\"\"\n    if isinstance(v, SkillSet):\n        return v\n    elif isinstance(v, Skill):\n        return LinearSkillSet(skills=[v])\n    else:\n        raise ValueError(f\"skills must be of type SkillSet or Skill, not {type(v)}\")\n</code></pre>"},{"location":"agents/#adala.agents.base.Agent.verify_input_parameters","title":"<code>verify_input_parameters()</code>","text":"<p>Verifies that the input parameters are valid.</p> Source code in <code>adala/agents/base.py</code> <pre><code>@model_validator(mode=\"after\")\ndef verify_input_parameters(self):\n    \"\"\"\n    Verifies that the input parameters are valid.\"\"\"\n\n    def _raise_default_runtime_error(val, runtime, runtimes, default_value):\n        print_error(\n            f\"The Agent.{runtime} is set to {val}, \"\n            f\"but this runtime is not available in the list: {list(runtimes)}. \"\n            f\"Please choose one of the available runtimes and initialize the agent again, for example:\\n\\n\"\n            f\"agent = Agent(..., {runtime}='{default_value}')\\n\\n\"\n            f\"Make sure the default runtime is available in the list of runtimes. For example:\\n\\n\"\n            f\"agent = Agent(..., runtimes={{'{default_value}': OpenAIRuntime(model='gpt-4')}})\\n\\n\"\n        )\n        raise ValueError(f\"default runtime {val} not found in provided runtimes.\")\n\n    if self.default_runtime not in self.runtimes:\n        _raise_default_runtime_error(\n            self.default_runtime, \"default_runtime\", self.runtimes, \"openai\"\n        )\n    if self.default_teacher_runtime not in self.teacher_runtimes:\n        _raise_default_runtime_error(\n            self.default_teacher_runtime,\n            \"default_teacher_runtime\",\n            self.teacher_runtimes,\n            \"openai-gpt4\",\n        )\n    return self\n</code></pre>"},{"location":"environments/","title":"Environments","text":""},{"location":"environments/#adala.environments.base.Environment","title":"<code>Environment</code>","text":"<p>             Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>An abstract base class that defines the structure and required methods for an environment in which machine learning models operate and are evaluated against ground truth data.</p> <p>Subclasses should implement methods to handle feedback requests, comparison to ground truth, dataset conversion, and state persistence.</p> Source code in <code>adala/environments/base.py</code> <pre><code>class Environment(BaseModel, ABC):\n    \"\"\"\n    An abstract base class that defines the structure and required methods for an environment\n    in which machine learning models operate and are evaluated against ground truth data.\n\n    Subclasses should implement methods to handle feedback requests, comparison to ground truth,\n    dataset conversion, and state persistence.\n    \"\"\"\n\n    @abstractmethod\n    def get_data_batch(self, batch_size=None) -&gt; InternalDataFrame:\n        \"\"\"\n        Get a batch of data from data stream to be processed by the skill set.\n\n        Args:\n            batch_size (Optional[int], optional): The size of the batch. Defaults to None\n\n        Returns:\n            InternalDataFrame: The data batch.\n        \"\"\"\n\n    @abstractmethod\n    def get_feedback(\n        self,\n        skills: SkillSet,\n        predictions: InternalDataFrame,\n        num_feedbacks: Optional[int] = None,\n    ) -&gt; EnvironmentFeedback:\n        \"\"\"\n        Request feedback for the predictions.\n\n        Args:\n            skills (SkillSet): The set of skills/models whose predictions are being evaluated.\n            predictions (InternalDataFrame): The predictions to compare with the ground truth.\n            num_feedbacks (Optional[int], optional): The number of feedbacks to request. Defaults to all predictions\n        Returns:\n            EnvironmentFeedback: The resulting ground truth signal, with matches and errors detailed.\n        \"\"\"\n\n    @abstractmethod\n    def save(self):\n        \"\"\"\n        Save the current state of the BasicEnvironment.\n\n        Raises:\n            NotImplementedError: This method is not implemented for BasicEnvironment.\n        \"\"\"\n\n    @abstractmethod\n    def restore(self):\n        \"\"\"\n        Restore the state of the BasicEnvironment.\n\n        Raises:\n            NotImplementedError: This method is not implemented for BasicEnvironment.\n        \"\"\"\n\n    class Config:\n        arbitrary_types_allowed = True\n</code></pre>"},{"location":"environments/#adala.environments.base.Environment.get_data_batch","title":"<code>get_data_batch(batch_size=None)</code>  <code>abstractmethod</code>","text":"<p>Get a batch of data from data stream to be processed by the skill set.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>Optional[int]</code> <p>The size of the batch. Defaults to None</p> <code>None</code> <p>Returns:</p> Name Type Description <code>InternalDataFrame</code> <code>InternalDataFrame</code> <p>The data batch.</p> Source code in <code>adala/environments/base.py</code> <pre><code>@abstractmethod\ndef get_data_batch(self, batch_size=None) -&gt; InternalDataFrame:\n    \"\"\"\n    Get a batch of data from data stream to be processed by the skill set.\n\n    Args:\n        batch_size (Optional[int], optional): The size of the batch. Defaults to None\n\n    Returns:\n        InternalDataFrame: The data batch.\n    \"\"\"\n</code></pre>"},{"location":"environments/#adala.environments.base.Environment.get_feedback","title":"<code>get_feedback(skills, predictions, num_feedbacks=None)</code>  <code>abstractmethod</code>","text":"<p>Request feedback for the predictions.</p> <p>Parameters:</p> Name Type Description Default <code>skills</code> <code>SkillSet</code> <p>The set of skills/models whose predictions are being evaluated.</p> required <code>predictions</code> <code>InternalDataFrame</code> <p>The predictions to compare with the ground truth.</p> required <code>num_feedbacks</code> <code>Optional[int]</code> <p>The number of feedbacks to request. Defaults to all predictions</p> <code>None</code> <p>Returns:     EnvironmentFeedback: The resulting ground truth signal, with matches and errors detailed.</p> Source code in <code>adala/environments/base.py</code> <pre><code>@abstractmethod\ndef get_feedback(\n    self,\n    skills: SkillSet,\n    predictions: InternalDataFrame,\n    num_feedbacks: Optional[int] = None,\n) -&gt; EnvironmentFeedback:\n    \"\"\"\n    Request feedback for the predictions.\n\n    Args:\n        skills (SkillSet): The set of skills/models whose predictions are being evaluated.\n        predictions (InternalDataFrame): The predictions to compare with the ground truth.\n        num_feedbacks (Optional[int], optional): The number of feedbacks to request. Defaults to all predictions\n    Returns:\n        EnvironmentFeedback: The resulting ground truth signal, with matches and errors detailed.\n    \"\"\"\n</code></pre>"},{"location":"environments/#adala.environments.base.Environment.restore","title":"<code>restore()</code>  <code>abstractmethod</code>","text":"<p>Restore the state of the BasicEnvironment.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method is not implemented for BasicEnvironment.</p> Source code in <code>adala/environments/base.py</code> <pre><code>@abstractmethod\ndef restore(self):\n    \"\"\"\n    Restore the state of the BasicEnvironment.\n\n    Raises:\n        NotImplementedError: This method is not implemented for BasicEnvironment.\n    \"\"\"\n</code></pre>"},{"location":"environments/#adala.environments.base.Environment.save","title":"<code>save()</code>  <code>abstractmethod</code>","text":"<p>Save the current state of the BasicEnvironment.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method is not implemented for BasicEnvironment.</p> Source code in <code>adala/environments/base.py</code> <pre><code>@abstractmethod\ndef save(self):\n    \"\"\"\n    Save the current state of the BasicEnvironment.\n\n    Raises:\n        NotImplementedError: This method is not implemented for BasicEnvironment.\n    \"\"\"\n</code></pre>"},{"location":"environments/#adala.environments.base.EnvironmentFeedback","title":"<code>EnvironmentFeedback</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>A class that represents the feedback received from an environment, along with the calculated correctness of predictions.</p> <p>Attributes:</p> Name Type Description <code>match</code> <code>InternalDataFrame</code> <p>A DataFrame indicating the correctness of predictions.                        Each row corresponds to a prediction, and each column is a boolean indicating if skill matches ground truth.                        Columns are named after the skill names.                        Indices correspond to prediction indices.                        Example:                            <code>| index | skill_1 | skill_2 | skill_3 |                             |-------|---------|---------|---------|                             | 0     | True    | True    | False   |                             | 1     | False   | False   | False   |                             | 2     | True    | True    | True    |</code></p> <code>feedback</code> <code>InternalDataFrame</code> <p>A DataFrame that contains ground truth feedback per each skill output</p> Source code in <code>adala/environments/base.py</code> <pre><code>class EnvironmentFeedback(BaseModel):\n    \"\"\"\n    A class that represents the feedback received from an environment,\n    along with the calculated correctness of predictions.\n\n    Attributes:\n        match (InternalDataFrame): A DataFrame indicating the correctness of predictions.\n                                   Each row corresponds to a prediction, and each column is a boolean indicating if skill matches ground truth.\n                                   Columns are named after the skill names.\n                                   Indices correspond to prediction indices.\n                                   Example:\n                                       ```\n                                        | index | skill_1 | skill_2 | skill_3 |\n                                        |-------|---------|---------|---------|\n                                        | 0     | True    | True    | False   |\n                                        | 1     | False   | False   | False   |\n                                        | 2     | True    | True    | True    |\n                                        ```\n        feedback (InternalDataFrame): A DataFrame that contains ground truth feedback per each skill output\n    \"\"\"\n\n    match: InternalDataFrame\n    feedback: InternalDataFrame\n\n    class Config:\n        arbitrary_types_allowed = True\n\n    def get_accuracy(self) -&gt; InternalSeries:\n        \"\"\"\n        Calculate the accuracy of predictions as the mean of matches.\n\n        Returns:\n            InternalSeries: A series representing the accuracy of predictions.\n        \"\"\"\n        return self.match.mean()\n\n    def __rich__(self):\n        text = \"[bold blue]Environment Feedback:[/bold blue]\\n\\n\"\n        text += f\"\\n[bold]Match[/bold]\\n{self.match}\"\n        if self.feedback is not None:\n            text += f\"\\n[bold]Feedback[/bold]\\n{self.feedback}\"\n        return text\n</code></pre>"},{"location":"environments/#adala.environments.base.EnvironmentFeedback.get_accuracy","title":"<code>get_accuracy()</code>","text":"<p>Calculate the accuracy of predictions as the mean of matches.</p> <p>Returns:</p> Name Type Description <code>InternalSeries</code> <code>InternalSeries</code> <p>A series representing the accuracy of predictions.</p> Source code in <code>adala/environments/base.py</code> <pre><code>def get_accuracy(self) -&gt; InternalSeries:\n    \"\"\"\n    Calculate the accuracy of predictions as the mean of matches.\n\n    Returns:\n        InternalSeries: A series representing the accuracy of predictions.\n    \"\"\"\n    return self.match.mean()\n</code></pre>"},{"location":"environments/#adala.environments.base.StaticEnvironment","title":"<code>StaticEnvironment</code>","text":"<p>             Bases: <code>Environment</code></p> <p>Static environment that initializes everything from the dataframe and doesn't not require requesting feedback to create the ground truth.</p> <p>Attributes     df (InternalDataFrame): The dataframe containing the ground truth.     ground_truth_columns ([Dict[str, str]]):         A dictionary mapping skill outputs to ground truth columns.         If not specified, the skill outputs are assumed to be the ground truth columns.         If a skill output is not in the dictionary, it is assumed to have no ground truth signal - NaNs are returned in the feedback.     matching_function (str, optional): The matching function to match ground truth strings with prediction strings.                                        Defaults to 'fuzzy'.     matching_threshold (float, optional): The matching threshold for the matching function.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame({'skill_1': ['a', 'b', 'c'], 'skill_2': ['d', 'e', 'f'], 'skill_3': ['g', 'h', 'i']})\n&gt;&gt;&gt; env = StaticEnvironment(df, ground_truth_columns={'skill_1': 'ground_truth_1', 'skill_2': 'ground_truth_2'})\n</code></pre> Source code in <code>adala/environments/base.py</code> <pre><code>class StaticEnvironment(Environment):\n    \"\"\"\n    Static environment that initializes everything from the dataframe\n    and doesn't not require requesting feedback to create the ground truth.\n\n    Attributes\n        df (InternalDataFrame): The dataframe containing the ground truth.\n        ground_truth_columns ([Dict[str, str]]):\n            A dictionary mapping skill outputs to ground truth columns.\n            If not specified, the skill outputs are assumed to be the ground truth columns.\n            If a skill output is not in the dictionary, it is assumed to have no ground truth signal - NaNs are returned in the feedback.\n        matching_function (str, optional): The matching function to match ground truth strings with prediction strings.\n                                           Defaults to 'fuzzy'.\n        matching_threshold (float, optional): The matching threshold for the matching function.\n\n    Examples:\n        &gt;&gt;&gt; df = pd.DataFrame({'skill_1': ['a', 'b', 'c'], 'skill_2': ['d', 'e', 'f'], 'skill_3': ['g', 'h', 'i']})\n        &gt;&gt;&gt; env = StaticEnvironment(df, ground_truth_columns={'skill_1': 'ground_truth_1', 'skill_2': 'ground_truth_2'})\n    \"\"\"\n\n    df: InternalDataFrame = None\n    ground_truth_columns: Dict[str, str] = Field(default_factory=dict)\n    matching_function: Union[str, Callable] = \"fuzzy\"\n    matching_threshold: float = 0.9\n\n    def get_feedback(\n        self,\n        skills: SkillSet,\n        predictions: InternalDataFrame,\n        num_feedbacks: Optional[int] = None,\n    ) -&gt; EnvironmentFeedback:\n        \"\"\"\n        Compare the predictions with the ground truth using the specified matching function.\n\n        Args:\n            skills (SkillSet): The skill set being evaluated.\n            predictions (InternalDataFrame): The predictions to compare with the ground truth.\n            num_feedbacks (Optional[int], optional): The number of feedbacks to request. Defaults to all predictions\n\n        Returns:\n            EnvironmentFeedback: The resulting ground truth signal, with matches and errors detailed.\n\n        Raises:\n            NotImplementedError: If the matching_function is unknown.\n        \"\"\"\n\n        pred_columns = list(skills.get_skill_outputs())\n        pred_match = {}\n        pred_feedback = {}\n\n        if num_feedbacks is not None:\n            predictions = predictions.sample(n=num_feedbacks)\n\n        for pred_column in pred_columns:\n            pred = predictions[pred_column]\n            gt_column = self.ground_truth_columns.get(pred_column, pred_column)\n            if gt_column not in self.df.columns:\n                # if ground truth column is not in the dataframe, assume no ground truth signal - return NaNs\n                pred_match[pred_column] = InternalSeries(np.nan, index=pred.index)\n                pred_feedback[pred_column] = InternalSeries(np.nan, index=pred.index)\n                continue\n\n            gt = self.df[gt_column]\n\n            gt, pred = gt.align(pred)\n            nonnull_index = gt.notnull() &amp; pred.notnull()\n            gt = gt[nonnull_index]\n            pred = pred[nonnull_index]\n            # compare ground truth with predictions\n            if isinstance(self.matching_function, str):\n                if self.matching_function == \"exact\":\n                    gt_pred_match = gt == pred\n                elif self.matching_function == \"fuzzy\":\n                    gt_pred_match = fuzzy_match(\n                        gt, pred, threshold=self.matching_threshold\n                    )\n                else:\n                    raise NotImplementedError(\n                        f\"Unknown matching function {self.matching_function}\"\n                    )\n            elif callable(self.matching_function):\n                gt_pred_match = gt.combine(\n                    pred, lambda g, p: self.matching_function(g, p)\n                )\n            pred_match[pred_column] = gt_pred_match\n            # leave feedback about mismatches\n            match_concat = InternalDataFrameConcat(\n                [gt_pred_match.rename(\"match\"), gt], axis=1\n            )\n            pred_feedback[pred_column] = match_concat.apply(\n                lambda row: \"Prediction is correct.\"\n                if row[\"match\"]\n                else f'Prediction is incorrect. Correct answer: \"{row[gt_column]}\"'\n                if not pd.isna(row[\"match\"])\n                else np.nan,\n                axis=1,\n            )\n\n        fb = EnvironmentFeedback(\n            match=InternalDataFrame(pred_match).reindex(predictions.index),\n            feedback=InternalDataFrame(pred_feedback).reindex(predictions.index),\n        )\n        return fb\n\n    def get_data_batch(self, batch_size: int = None) -&gt; InternalDataFrame:\n        \"\"\"\n        Return the dataset containing the ground truth data.\n\n        Returns:\n            InternalDataFrame: The data batch.\n        \"\"\"\n        if batch_size is not None:\n            return self.df.sample(n=batch_size)\n        return self.df\n\n    def save(self):\n        \"\"\"\n        Save the current state of the StaticEnvironment.\n        \"\"\"\n        raise NotImplementedError(\"StaticEnvironment does not support save/restore.\")\n\n    def restore(self):\n        \"\"\"\n        Restore the state of the StaticEnvironment.\n        \"\"\"\n        raise NotImplementedError(\"StaticEnvironment does not support save/restore.\")\n</code></pre>"},{"location":"environments/#adala.environments.base.StaticEnvironment.get_data_batch","title":"<code>get_data_batch(batch_size=None)</code>","text":"<p>Return the dataset containing the ground truth data.</p> <p>Returns:</p> Name Type Description <code>InternalDataFrame</code> <code>InternalDataFrame</code> <p>The data batch.</p> Source code in <code>adala/environments/base.py</code> <pre><code>def get_data_batch(self, batch_size: int = None) -&gt; InternalDataFrame:\n    \"\"\"\n    Return the dataset containing the ground truth data.\n\n    Returns:\n        InternalDataFrame: The data batch.\n    \"\"\"\n    if batch_size is not None:\n        return self.df.sample(n=batch_size)\n    return self.df\n</code></pre>"},{"location":"environments/#adala.environments.base.StaticEnvironment.get_feedback","title":"<code>get_feedback(skills, predictions, num_feedbacks=None)</code>","text":"<p>Compare the predictions with the ground truth using the specified matching function.</p> <p>Parameters:</p> Name Type Description Default <code>skills</code> <code>SkillSet</code> <p>The skill set being evaluated.</p> required <code>predictions</code> <code>InternalDataFrame</code> <p>The predictions to compare with the ground truth.</p> required <code>num_feedbacks</code> <code>Optional[int]</code> <p>The number of feedbacks to request. Defaults to all predictions</p> <code>None</code> <p>Returns:</p> Name Type Description <code>EnvironmentFeedback</code> <code>EnvironmentFeedback</code> <p>The resulting ground truth signal, with matches and errors detailed.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the matching_function is unknown.</p> Source code in <code>adala/environments/base.py</code> <pre><code>def get_feedback(\n    self,\n    skills: SkillSet,\n    predictions: InternalDataFrame,\n    num_feedbacks: Optional[int] = None,\n) -&gt; EnvironmentFeedback:\n    \"\"\"\n    Compare the predictions with the ground truth using the specified matching function.\n\n    Args:\n        skills (SkillSet): The skill set being evaluated.\n        predictions (InternalDataFrame): The predictions to compare with the ground truth.\n        num_feedbacks (Optional[int], optional): The number of feedbacks to request. Defaults to all predictions\n\n    Returns:\n        EnvironmentFeedback: The resulting ground truth signal, with matches and errors detailed.\n\n    Raises:\n        NotImplementedError: If the matching_function is unknown.\n    \"\"\"\n\n    pred_columns = list(skills.get_skill_outputs())\n    pred_match = {}\n    pred_feedback = {}\n\n    if num_feedbacks is not None:\n        predictions = predictions.sample(n=num_feedbacks)\n\n    for pred_column in pred_columns:\n        pred = predictions[pred_column]\n        gt_column = self.ground_truth_columns.get(pred_column, pred_column)\n        if gt_column not in self.df.columns:\n            # if ground truth column is not in the dataframe, assume no ground truth signal - return NaNs\n            pred_match[pred_column] = InternalSeries(np.nan, index=pred.index)\n            pred_feedback[pred_column] = InternalSeries(np.nan, index=pred.index)\n            continue\n\n        gt = self.df[gt_column]\n\n        gt, pred = gt.align(pred)\n        nonnull_index = gt.notnull() &amp; pred.notnull()\n        gt = gt[nonnull_index]\n        pred = pred[nonnull_index]\n        # compare ground truth with predictions\n        if isinstance(self.matching_function, str):\n            if self.matching_function == \"exact\":\n                gt_pred_match = gt == pred\n            elif self.matching_function == \"fuzzy\":\n                gt_pred_match = fuzzy_match(\n                    gt, pred, threshold=self.matching_threshold\n                )\n            else:\n                raise NotImplementedError(\n                    f\"Unknown matching function {self.matching_function}\"\n                )\n        elif callable(self.matching_function):\n            gt_pred_match = gt.combine(\n                pred, lambda g, p: self.matching_function(g, p)\n            )\n        pred_match[pred_column] = gt_pred_match\n        # leave feedback about mismatches\n        match_concat = InternalDataFrameConcat(\n            [gt_pred_match.rename(\"match\"), gt], axis=1\n        )\n        pred_feedback[pred_column] = match_concat.apply(\n            lambda row: \"Prediction is correct.\"\n            if row[\"match\"]\n            else f'Prediction is incorrect. Correct answer: \"{row[gt_column]}\"'\n            if not pd.isna(row[\"match\"])\n            else np.nan,\n            axis=1,\n        )\n\n    fb = EnvironmentFeedback(\n        match=InternalDataFrame(pred_match).reindex(predictions.index),\n        feedback=InternalDataFrame(pred_feedback).reindex(predictions.index),\n    )\n    return fb\n</code></pre>"},{"location":"environments/#adala.environments.base.StaticEnvironment.restore","title":"<code>restore()</code>","text":"<p>Restore the state of the StaticEnvironment.</p> Source code in <code>adala/environments/base.py</code> <pre><code>def restore(self):\n    \"\"\"\n    Restore the state of the StaticEnvironment.\n    \"\"\"\n    raise NotImplementedError(\"StaticEnvironment does not support save/restore.\")\n</code></pre>"},{"location":"environments/#adala.environments.base.StaticEnvironment.save","title":"<code>save()</code>","text":"<p>Save the current state of the StaticEnvironment.</p> Source code in <code>adala/environments/base.py</code> <pre><code>def save(self):\n    \"\"\"\n    Save the current state of the StaticEnvironment.\n    \"\"\"\n    raise NotImplementedError(\"StaticEnvironment does not support save/restore.\")\n</code></pre>"},{"location":"memories/","title":"Memories","text":""},{"location":"memories/#adala.memories.base.Memory","title":"<code>Memory</code>","text":"<p>             Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Base class for memories.</p> Source code in <code>adala/memories/base.py</code> <pre><code>class Memory(BaseModel, ABC):\n\n    \"\"\"\n    Base class for memories.\n    \"\"\"\n\n    @abstractmethod\n    def remember(self, observation: str, data: Dict):\n        \"\"\"\n        Base method for remembering experiences in long term memory.\n        \"\"\"\n\n    def remember_many(self, observations: List[str], data: List[Dict]):\n        \"\"\"\n        Base method for remembering experiences in long term memory.\n        \"\"\"\n        for observation, d in zip(observations, data):\n            self.remember(observation, d)\n\n    @abstractmethod\n    def retrieve(self, observation: str, num_results: int = 1) -&gt; Any:\n        \"\"\"\n        Base method for retrieving past experiences from long term memory, based on current observations\n\n        Args:\n            observation: the current observation\n            num_results: the number of results to return\n        \"\"\"\n\n    def retrieve_many(self, observations: List[str], num_results: int = 1) -&gt; List[Any]:\n        \"\"\"\n        Base method for retrieving past experiences from long term memory, based on current observations\n\n        Args:\n            observation: the current observation\n            num_results: the number of results to return\n        \"\"\"\n        return [self.retrieve(observation) for observation in observations]\n\n    @abstractmethod\n    def clear(self):\n        \"\"\"\n        Base method for clearing memory.\n        \"\"\"\n</code></pre>"},{"location":"memories/#adala.memories.base.Memory.clear","title":"<code>clear()</code>  <code>abstractmethod</code>","text":"<p>Base method for clearing memory.</p> Source code in <code>adala/memories/base.py</code> <pre><code>@abstractmethod\ndef clear(self):\n    \"\"\"\n    Base method for clearing memory.\n    \"\"\"\n</code></pre>"},{"location":"memories/#adala.memories.base.Memory.remember","title":"<code>remember(observation, data)</code>  <code>abstractmethod</code>","text":"<p>Base method for remembering experiences in long term memory.</p> Source code in <code>adala/memories/base.py</code> <pre><code>@abstractmethod\ndef remember(self, observation: str, data: Dict):\n    \"\"\"\n    Base method for remembering experiences in long term memory.\n    \"\"\"\n</code></pre>"},{"location":"memories/#adala.memories.base.Memory.remember_many","title":"<code>remember_many(observations, data)</code>","text":"<p>Base method for remembering experiences in long term memory.</p> Source code in <code>adala/memories/base.py</code> <pre><code>def remember_many(self, observations: List[str], data: List[Dict]):\n    \"\"\"\n    Base method for remembering experiences in long term memory.\n    \"\"\"\n    for observation, d in zip(observations, data):\n        self.remember(observation, d)\n</code></pre>"},{"location":"memories/#adala.memories.base.Memory.retrieve","title":"<code>retrieve(observation, num_results=1)</code>  <code>abstractmethod</code>","text":"<p>Base method for retrieving past experiences from long term memory, based on current observations</p> <p>Parameters:</p> Name Type Description Default <code>observation</code> <code>str</code> <p>the current observation</p> required <code>num_results</code> <code>int</code> <p>the number of results to return</p> <code>1</code> Source code in <code>adala/memories/base.py</code> <pre><code>@abstractmethod\ndef retrieve(self, observation: str, num_results: int = 1) -&gt; Any:\n    \"\"\"\n    Base method for retrieving past experiences from long term memory, based on current observations\n\n    Args:\n        observation: the current observation\n        num_results: the number of results to return\n    \"\"\"\n</code></pre>"},{"location":"memories/#adala.memories.base.Memory.retrieve_many","title":"<code>retrieve_many(observations, num_results=1)</code>","text":"<p>Base method for retrieving past experiences from long term memory, based on current observations</p> <p>Parameters:</p> Name Type Description Default <code>observation</code> <p>the current observation</p> required <code>num_results</code> <code>int</code> <p>the number of results to return</p> <code>1</code> Source code in <code>adala/memories/base.py</code> <pre><code>def retrieve_many(self, observations: List[str], num_results: int = 1) -&gt; List[Any]:\n    \"\"\"\n    Base method for retrieving past experiences from long term memory, based on current observations\n\n    Args:\n        observation: the current observation\n        num_results: the number of results to return\n    \"\"\"\n    return [self.retrieve(observation) for observation in observations]\n</code></pre>"},{"location":"memories/#adala.memories.file_memory.FileMemory","title":"<code>FileMemory</code>","text":"<p>             Bases: <code>Memory</code></p> Source code in <code>adala/memories/file_memory.py</code> <pre><code>class FileMemory(Memory):\n    filepath: str\n\n    def remember(self, observation: str, experience: Any):\n        \"\"\"\n        Serialize experience in JSON and append to file\n        \"\"\"\n        with open(self.filepath) as f:\n            memory = json.load(f)\n        memory[observation] = experience\n        with open(self.filepath, \"w\") as f:\n            json.dump(memory, f, indent=2)\n\n    def retrieve(self, observation: str) -&gt; Any:\n        \"\"\"\n        Retrieve experience from file\n        \"\"\"\n        with open(self.filepath) as f:\n            memory = json.load(f)\n        return memory[observation]\n</code></pre>"},{"location":"memories/#adala.memories.file_memory.FileMemory.remember","title":"<code>remember(observation, experience)</code>","text":"<p>Serialize experience in JSON and append to file</p> Source code in <code>adala/memories/file_memory.py</code> <pre><code>def remember(self, observation: str, experience: Any):\n    \"\"\"\n    Serialize experience in JSON and append to file\n    \"\"\"\n    with open(self.filepath) as f:\n        memory = json.load(f)\n    memory[observation] = experience\n    with open(self.filepath, \"w\") as f:\n        json.dump(memory, f, indent=2)\n</code></pre>"},{"location":"memories/#adala.memories.file_memory.FileMemory.retrieve","title":"<code>retrieve(observation)</code>","text":"<p>Retrieve experience from file</p> Source code in <code>adala/memories/file_memory.py</code> <pre><code>def retrieve(self, observation: str) -&gt; Any:\n    \"\"\"\n    Retrieve experience from file\n    \"\"\"\n    with open(self.filepath) as f:\n        memory = json.load(f)\n    return memory[observation]\n</code></pre>"},{"location":"runtimes/","title":"Runtimes","text":""},{"location":"runtimes/#adala.runtimes.base.Runtime","title":"<code>Runtime</code>","text":"<p>             Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Base class representing a generic runtime environment.</p> <p>Attributes:</p> Name Type Description <code>verbose</code> <code>bool</code> <p>Flag indicating if runtime outputs should be verbose. Defaults to False.</p> Source code in <code>adala/runtimes/base.py</code> <pre><code>class Runtime(BaseModel, ABC):\n    \"\"\"\n    Base class representing a generic runtime environment.\n\n    Attributes:\n        verbose (bool): Flag indicating if runtime outputs should be verbose. Defaults to False.\n    \"\"\"\n\n    verbose: bool = False\n\n    @model_validator(mode=\"after\")\n    def init_runtime(self) -&gt; \"Runtime\":\n        \"\"\"Initializes the runtime.\n\n        This method should be used to validate and potentially initialize the runtime instance.\n\n        Returns:\n            Runtime: The initialized runtime instance.\n        \"\"\"\n        return self\n\n    @abstractmethod\n    def record_to_record(\n        self,\n        record: Dict[str, str],\n        input_template: str,\n        instructions_template: str,\n        output_template: str,\n        extra_fields: Optional[Dict[str, Any]] = None,\n        field_schema: Optional[Dict] = None,\n        instructions_first: bool = True,\n    ) -&gt; Dict[str, str]:\n        \"\"\"\n        Processes a record.\n\n        Args:\n            record (Dict[str, str]): The record to process.\n            input_template (str): The input template.\n            instructions_template (str): The instructions template.\n            output_template (str): The output template.\n            extra_fields (Optional[Dict[str, str]]): Extra fields to use in the templates. Defaults to None.\n            field_schema (Optional[Dict]): Field JSON schema to use in the templates. Defaults to all fields are strings,\n                i.e. analogous to {\"field_n\": {\"type\": \"string\"}}.\n            instructions_first (bool): Whether to put instructions first. Defaults to True.\n\n        Returns:\n            Dict[str, str]: The processed record.\n        \"\"\"\n\n    def batch_to_batch(\n        self,\n        batch: InternalDataFrame,\n        input_template: str,\n        instructions_template: str,\n        output_template: str,\n        extra_fields: Optional[Dict[str, str]] = None,\n        field_schema: Optional[Dict] = None,\n        instructions_first: bool = True,\n    ) -&gt; InternalDataFrame:\n        \"\"\"\n        Processes a record.\n\n        Args:\n            batch (InternalDataFrame): The batch to process.\n            input_template (str): The input template.\n            instructions_template (str): The instructions template.\n            output_template (str): The output template.\n            extra_fields (Optional[Dict[str, str]]): Extra fields to use in the templates. Defaults to None.\n            field_schema (Optional[Dict]): Field JSON schema to use in the templates. Defaults to all fields are strings,\n                i.e. analogous to {\"field_n\": {\"type\": \"string\"}}.\n            instructions_first (bool): Whether to put instructions first. Defaults to True.\n\n        Returns:\n            InternalDataFrame: The processed batch.\n        \"\"\"\n        output = batch.progress_apply(\n            self.record_to_record,\n            axis=1,\n            result_type=\"expand\",\n            input_template=input_template,\n            instructions_template=instructions_template,\n            output_template=output_template,\n            extra_fields=extra_fields,\n            field_schema=field_schema,\n            instructions_first=instructions_first,\n        )\n        return output\n\n    def record_to_batch(\n        self,\n        record: Dict[str, str],\n        input_template: str,\n        instructions_template: str,\n        output_template: str,\n        output_batch_size: int = 1,\n        extra_fields: Optional[Dict[str, str]] = None,\n        field_schema: Optional[Dict] = None,\n        instructions_first: bool = True,\n    ) -&gt; InternalDataFrame:\n        \"\"\"\n        Processes a record and return a batch.\n\n        Args:\n            record (Dict[str, str]): The record to process.\n            input_template (str): The input template.\n            instructions_template (str): The instructions template.\n            output_template (str): The output template.\n            output_batch_size (int): The batch size for the output. Defaults to 1.\n            extra_fields (Optional[Dict[str, str]]): Extra fields to use in the templates. Defaults to None.\n            field_schema (Optional[Dict]): Field JSON schema to use in the templates. Defaults to all fields are strings,\n                i.e. analogous to {\"field_n\": {\"type\": \"string\"}}.\n            instructions_first (bool): Whether to put instructions first. Defaults to True.\n\n        Returns:\n            InternalDataFrame: The processed batch.\n        \"\"\"\n        batch = InternalDataFrame([record] * output_batch_size)\n        return self.batch_to_batch(\n            batch=batch,\n            input_template=input_template,\n            instructions_template=instructions_template,\n            output_template=output_template,\n            extra_fields=extra_fields,\n            field_schema=field_schema,\n            instructions_first=instructions_first,\n        )\n</code></pre>"},{"location":"runtimes/#adala.runtimes.base.Runtime.batch_to_batch","title":"<code>batch_to_batch(batch, input_template, instructions_template, output_template, extra_fields=None, field_schema=None, instructions_first=True)</code>","text":"<p>Processes a record.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>InternalDataFrame</code> <p>The batch to process.</p> required <code>input_template</code> <code>str</code> <p>The input template.</p> required <code>instructions_template</code> <code>str</code> <p>The instructions template.</p> required <code>output_template</code> <code>str</code> <p>The output template.</p> required <code>extra_fields</code> <code>Optional[Dict[str, str]]</code> <p>Extra fields to use in the templates. Defaults to None.</p> <code>None</code> <code>field_schema</code> <code>Optional[Dict]</code> <p>Field JSON schema to use in the templates. Defaults to all fields are strings, i.e. analogous to {\"field_n\": {\"type\": \"string\"}}.</p> <code>None</code> <code>instructions_first</code> <code>bool</code> <p>Whether to put instructions first. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>InternalDataFrame</code> <code>InternalDataFrame</code> <p>The processed batch.</p> Source code in <code>adala/runtimes/base.py</code> <pre><code>def batch_to_batch(\n    self,\n    batch: InternalDataFrame,\n    input_template: str,\n    instructions_template: str,\n    output_template: str,\n    extra_fields: Optional[Dict[str, str]] = None,\n    field_schema: Optional[Dict] = None,\n    instructions_first: bool = True,\n) -&gt; InternalDataFrame:\n    \"\"\"\n    Processes a record.\n\n    Args:\n        batch (InternalDataFrame): The batch to process.\n        input_template (str): The input template.\n        instructions_template (str): The instructions template.\n        output_template (str): The output template.\n        extra_fields (Optional[Dict[str, str]]): Extra fields to use in the templates. Defaults to None.\n        field_schema (Optional[Dict]): Field JSON schema to use in the templates. Defaults to all fields are strings,\n            i.e. analogous to {\"field_n\": {\"type\": \"string\"}}.\n        instructions_first (bool): Whether to put instructions first. Defaults to True.\n\n    Returns:\n        InternalDataFrame: The processed batch.\n    \"\"\"\n    output = batch.progress_apply(\n        self.record_to_record,\n        axis=1,\n        result_type=\"expand\",\n        input_template=input_template,\n        instructions_template=instructions_template,\n        output_template=output_template,\n        extra_fields=extra_fields,\n        field_schema=field_schema,\n        instructions_first=instructions_first,\n    )\n    return output\n</code></pre>"},{"location":"runtimes/#adala.runtimes.base.Runtime.init_runtime","title":"<code>init_runtime()</code>","text":"<p>Initializes the runtime.</p> <p>This method should be used to validate and potentially initialize the runtime instance.</p> <p>Returns:</p> Name Type Description <code>Runtime</code> <code>Runtime</code> <p>The initialized runtime instance.</p> Source code in <code>adala/runtimes/base.py</code> <pre><code>@model_validator(mode=\"after\")\ndef init_runtime(self) -&gt; \"Runtime\":\n    \"\"\"Initializes the runtime.\n\n    This method should be used to validate and potentially initialize the runtime instance.\n\n    Returns:\n        Runtime: The initialized runtime instance.\n    \"\"\"\n    return self\n</code></pre>"},{"location":"runtimes/#adala.runtimes.base.Runtime.record_to_batch","title":"<code>record_to_batch(record, input_template, instructions_template, output_template, output_batch_size=1, extra_fields=None, field_schema=None, instructions_first=True)</code>","text":"<p>Processes a record and return a batch.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>Dict[str, str]</code> <p>The record to process.</p> required <code>input_template</code> <code>str</code> <p>The input template.</p> required <code>instructions_template</code> <code>str</code> <p>The instructions template.</p> required <code>output_template</code> <code>str</code> <p>The output template.</p> required <code>output_batch_size</code> <code>int</code> <p>The batch size for the output. Defaults to 1.</p> <code>1</code> <code>extra_fields</code> <code>Optional[Dict[str, str]]</code> <p>Extra fields to use in the templates. Defaults to None.</p> <code>None</code> <code>field_schema</code> <code>Optional[Dict]</code> <p>Field JSON schema to use in the templates. Defaults to all fields are strings, i.e. analogous to {\"field_n\": {\"type\": \"string\"}}.</p> <code>None</code> <code>instructions_first</code> <code>bool</code> <p>Whether to put instructions first. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>InternalDataFrame</code> <code>InternalDataFrame</code> <p>The processed batch.</p> Source code in <code>adala/runtimes/base.py</code> <pre><code>def record_to_batch(\n    self,\n    record: Dict[str, str],\n    input_template: str,\n    instructions_template: str,\n    output_template: str,\n    output_batch_size: int = 1,\n    extra_fields: Optional[Dict[str, str]] = None,\n    field_schema: Optional[Dict] = None,\n    instructions_first: bool = True,\n) -&gt; InternalDataFrame:\n    \"\"\"\n    Processes a record and return a batch.\n\n    Args:\n        record (Dict[str, str]): The record to process.\n        input_template (str): The input template.\n        instructions_template (str): The instructions template.\n        output_template (str): The output template.\n        output_batch_size (int): The batch size for the output. Defaults to 1.\n        extra_fields (Optional[Dict[str, str]]): Extra fields to use in the templates. Defaults to None.\n        field_schema (Optional[Dict]): Field JSON schema to use in the templates. Defaults to all fields are strings,\n            i.e. analogous to {\"field_n\": {\"type\": \"string\"}}.\n        instructions_first (bool): Whether to put instructions first. Defaults to True.\n\n    Returns:\n        InternalDataFrame: The processed batch.\n    \"\"\"\n    batch = InternalDataFrame([record] * output_batch_size)\n    return self.batch_to_batch(\n        batch=batch,\n        input_template=input_template,\n        instructions_template=instructions_template,\n        output_template=output_template,\n        extra_fields=extra_fields,\n        field_schema=field_schema,\n        instructions_first=instructions_first,\n    )\n</code></pre>"},{"location":"runtimes/#adala.runtimes.base.Runtime.record_to_record","title":"<code>record_to_record(record, input_template, instructions_template, output_template, extra_fields=None, field_schema=None, instructions_first=True)</code>  <code>abstractmethod</code>","text":"<p>Processes a record.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>Dict[str, str]</code> <p>The record to process.</p> required <code>input_template</code> <code>str</code> <p>The input template.</p> required <code>instructions_template</code> <code>str</code> <p>The instructions template.</p> required <code>output_template</code> <code>str</code> <p>The output template.</p> required <code>extra_fields</code> <code>Optional[Dict[str, str]]</code> <p>Extra fields to use in the templates. Defaults to None.</p> <code>None</code> <code>field_schema</code> <code>Optional[Dict]</code> <p>Field JSON schema to use in the templates. Defaults to all fields are strings, i.e. analogous to {\"field_n\": {\"type\": \"string\"}}.</p> <code>None</code> <code>instructions_first</code> <code>bool</code> <p>Whether to put instructions first. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dict[str, str]: The processed record.</p> Source code in <code>adala/runtimes/base.py</code> <pre><code>@abstractmethod\ndef record_to_record(\n    self,\n    record: Dict[str, str],\n    input_template: str,\n    instructions_template: str,\n    output_template: str,\n    extra_fields: Optional[Dict[str, Any]] = None,\n    field_schema: Optional[Dict] = None,\n    instructions_first: bool = True,\n) -&gt; Dict[str, str]:\n    \"\"\"\n    Processes a record.\n\n    Args:\n        record (Dict[str, str]): The record to process.\n        input_template (str): The input template.\n        instructions_template (str): The instructions template.\n        output_template (str): The output template.\n        extra_fields (Optional[Dict[str, str]]): Extra fields to use in the templates. Defaults to None.\n        field_schema (Optional[Dict]): Field JSON schema to use in the templates. Defaults to all fields are strings,\n            i.e. analogous to {\"field_n\": {\"type\": \"string\"}}.\n        instructions_first (bool): Whether to put instructions first. Defaults to True.\n\n    Returns:\n        Dict[str, str]: The processed record.\n    \"\"\"\n</code></pre>"},{"location":"runtimes/#adala.runtimes._openai.OpenAIChatRuntime","title":"<code>OpenAIChatRuntime</code>","text":"<p>             Bases: <code>Runtime</code></p> <p>Runtime that uses OpenAI API and chat completion models to perform the skill.</p> <p>Attributes:</p> Name Type Description <code>openai_model</code> <code>str</code> <p>OpenAI model name.</p> <code>openai_api_key</code> <code>Optional[str]</code> <p>OpenAI API key. If not provided, will be taken from OPENAI_API_KEY environment variable.</p> <code>max_tokens</code> <code>Optional[int]</code> <p>Maximum number of tokens to generate. Defaults to 1000.</p> Source code in <code>adala/runtimes/_openai.py</code> <pre><code>class OpenAIChatRuntime(Runtime):\n    \"\"\"\n    Runtime that uses [OpenAI API](https://openai.com/) and chat completion models to perform the skill.\n\n    Attributes:\n        openai_model: OpenAI model name.\n        openai_api_key: OpenAI API key. If not provided, will be taken from OPENAI_API_KEY environment variable.\n        max_tokens: Maximum number of tokens to generate. Defaults to 1000.\n    \"\"\"\n\n    openai_model: str = Field(alias=\"model\")\n    openai_api_key: Optional[str] = Field(\n        default=os.getenv(\"OPENAI_API_KEY\"), alias=\"api_key\"\n    )\n    max_tokens: Optional[int] = 1000\n    splitter: Optional[str] = None\n\n    _client: OpenAI = None\n\n    def init_runtime(self) -&gt; \"Runtime\":\n        # check openai package version\n        if check_if_new_openai_version():\n            if self._client is None:\n                self._client = OpenAI(api_key=self.openai_api_key)\n\n            # check model availability\n            try:\n                self._client.models.retrieve(self.openai_model)\n            except NotFoundError:\n                raise ValueError(\n                    f'Requested model \"{self.openai_model}\" is not available in your OpenAI account.'\n                )\n        else:\n            # deprecated\n            models = openai.Model.list(api_key=self.openai_api_key)\n            models = set(model[\"id\"] for model in models[\"data\"])\n            if self.openai_model not in models:\n                print_error(\n                    f'Requested model \"{self.openai_model}\" is not available in your OpenAI account. '\n                    f\"Available models are: {models}\\n\\n\"\n                    f\"Try to change the runtime settings for {self.__class__.__name__}, for example:\\n\\n\"\n                    f'{self.__class__.__name__}(..., model=\"gpt-3.5-turbo\")\\n\\n'\n                )\n                raise ValueError(\n                    f\"Requested model {self.openai_model} is not available in your OpenAI account.\"\n                )\n        return self\n\n    def execute(self, messages: List):\n        \"\"\"\n        Execute OpenAI request given list of messages in OpenAI API format\n        \"\"\"\n        if self.verbose:\n            print(f\"OpenAI request: {messages}\")\n\n        if check_if_new_openai_version():\n            completion = self._client.chat.completions.create(\n                model=self.openai_model, messages=messages\n            )\n            completion_text = completion.choices[0].message.content\n        else:\n            # deprecated\n            completion = chat_completion_call(self.openai_model, messages)\n            completion_text = completion.choices[0][\"message\"][\"content\"]\n        return completion_text\n\n    def record_to_record(\n        self,\n        record: Dict[str, str],\n        input_template: str,\n        instructions_template: str,\n        output_template: str,\n        extra_fields: Optional[Dict[str, str]] = None,\n        field_schema: Optional[Dict] = None,\n        instructions_first: bool = False,\n    ) -&gt; Dict[str, str]:\n        \"\"\"\n        Execute OpenAI request given record and templates for input, instructions and output.\n\n        Args:\n            record: Record to be used for input, instructions and output templates.\n            input_template: Template for input message.\n            instructions_template: Template for instructions message.\n            output_template: Template for output message.\n            extra_fields: Extra fields to be used in templates.\n            field_schema: Field schema to be used for parsing templates.\n            instructions_first: If True, instructions will be sent before input.\n\n        Returns:\n            Dict[str, str]: Output record.\n        \"\"\"\n\n        extra_fields = extra_fields or {}\n\n        output_fields = parse_template(\n            partial_str_format(output_template, **extra_fields), include_texts=False\n        )\n        if len(output_fields) &gt; 1:\n            raise NotImplementedError(\n                f\"{self.__class__.__name__} does not support multiple output fields. \"\n                f\"Found: {output_fields}\"\n            )\n        output_field = output_fields[0]\n        output_field_name = output_field[\"text\"]\n        system_prompt = instructions_template\n        user_prompt = input_template.format(**record, **extra_fields)\n        # TODO: this truncates the suffix of the output template\n        # for example, output template \"Output: {answer} is correct\" results in output_prefix \"Output: \"\n        output_prefix = output_template[: output_field[\"start\"]]\n        if instructions_first:\n            user_prompt += f\"\\n{output_prefix}\"\n            messages = [\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": user_prompt},\n            ]\n        else:\n            user_prompt = f\"{user_prompt}\\n\\n{system_prompt}\\n\\n{output_prefix}\"\n            messages = [\n                {\"role\": \"user\", \"content\": user_prompt},\n            ]\n\n        completion_text = self.execute(messages)\n\n        field_schema = field_schema or {}\n        if (\n            output_field_name in field_schema\n            and field_schema[output_field_name][\"type\"] == \"array\"\n        ):\n            # expected output is one item from the array\n            expected_items = field_schema[output_field_name][\"items\"][\"enum\"]\n            completion_text = self._match_items(completion_text, expected_items)\n\n        return {output_field_name: completion_text}\n\n    def _match_items(self, query: str, items: List[str]) -&gt; str:\n        # hard constraint: the item must be in the query\n        filtered_items = [item for item in items if item in query]\n        if not filtered_items:\n            # make the best guess - find the most similar item to the query\n            filtered_items = items\n\n        # soft constraint: find the most similar item to the query\n        matched_items = []\n        # split query by self.splitter\n        if self.splitter:\n            qs = query.split(self.splitter)\n        else:\n            qs = [query]\n\n        for q in qs:\n            scores = list(\n                map(\n                    lambda item: difflib.SequenceMatcher(None, q, item).ratio(),\n                    filtered_items,\n                )\n            )\n            matched_items.append(filtered_items[scores.index(max(scores))])\n        if self.splitter:\n            return self.splitter.join(matched_items)\n        return matched_items[0]\n</code></pre>"},{"location":"runtimes/#adala.runtimes._openai.OpenAIChatRuntime.execute","title":"<code>execute(messages)</code>","text":"<p>Execute OpenAI request given list of messages in OpenAI API format</p> Source code in <code>adala/runtimes/_openai.py</code> <pre><code>def execute(self, messages: List):\n    \"\"\"\n    Execute OpenAI request given list of messages in OpenAI API format\n    \"\"\"\n    if self.verbose:\n        print(f\"OpenAI request: {messages}\")\n\n    if check_if_new_openai_version():\n        completion = self._client.chat.completions.create(\n            model=self.openai_model, messages=messages\n        )\n        completion_text = completion.choices[0].message.content\n    else:\n        # deprecated\n        completion = chat_completion_call(self.openai_model, messages)\n        completion_text = completion.choices[0][\"message\"][\"content\"]\n    return completion_text\n</code></pre>"},{"location":"runtimes/#adala.runtimes._openai.OpenAIChatRuntime.record_to_record","title":"<code>record_to_record(record, input_template, instructions_template, output_template, extra_fields=None, field_schema=None, instructions_first=False)</code>","text":"<p>Execute OpenAI request given record and templates for input, instructions and output.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>Dict[str, str]</code> <p>Record to be used for input, instructions and output templates.</p> required <code>input_template</code> <code>str</code> <p>Template for input message.</p> required <code>instructions_template</code> <code>str</code> <p>Template for instructions message.</p> required <code>output_template</code> <code>str</code> <p>Template for output message.</p> required <code>extra_fields</code> <code>Optional[Dict[str, str]]</code> <p>Extra fields to be used in templates.</p> <code>None</code> <code>field_schema</code> <code>Optional[Dict]</code> <p>Field schema to be used for parsing templates.</p> <code>None</code> <code>instructions_first</code> <code>bool</code> <p>If True, instructions will be sent before input.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dict[str, str]: Output record.</p> Source code in <code>adala/runtimes/_openai.py</code> <pre><code>def record_to_record(\n    self,\n    record: Dict[str, str],\n    input_template: str,\n    instructions_template: str,\n    output_template: str,\n    extra_fields: Optional[Dict[str, str]] = None,\n    field_schema: Optional[Dict] = None,\n    instructions_first: bool = False,\n) -&gt; Dict[str, str]:\n    \"\"\"\n    Execute OpenAI request given record and templates for input, instructions and output.\n\n    Args:\n        record: Record to be used for input, instructions and output templates.\n        input_template: Template for input message.\n        instructions_template: Template for instructions message.\n        output_template: Template for output message.\n        extra_fields: Extra fields to be used in templates.\n        field_schema: Field schema to be used for parsing templates.\n        instructions_first: If True, instructions will be sent before input.\n\n    Returns:\n        Dict[str, str]: Output record.\n    \"\"\"\n\n    extra_fields = extra_fields or {}\n\n    output_fields = parse_template(\n        partial_str_format(output_template, **extra_fields), include_texts=False\n    )\n    if len(output_fields) &gt; 1:\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} does not support multiple output fields. \"\n            f\"Found: {output_fields}\"\n        )\n    output_field = output_fields[0]\n    output_field_name = output_field[\"text\"]\n    system_prompt = instructions_template\n    user_prompt = input_template.format(**record, **extra_fields)\n    # TODO: this truncates the suffix of the output template\n    # for example, output template \"Output: {answer} is correct\" results in output_prefix \"Output: \"\n    output_prefix = output_template[: output_field[\"start\"]]\n    if instructions_first:\n        user_prompt += f\"\\n{output_prefix}\"\n        messages = [\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt},\n        ]\n    else:\n        user_prompt = f\"{user_prompt}\\n\\n{system_prompt}\\n\\n{output_prefix}\"\n        messages = [\n            {\"role\": \"user\", \"content\": user_prompt},\n        ]\n\n    completion_text = self.execute(messages)\n\n    field_schema = field_schema or {}\n    if (\n        output_field_name in field_schema\n        and field_schema[output_field_name][\"type\"] == \"array\"\n    ):\n        # expected output is one item from the array\n        expected_items = field_schema[output_field_name][\"items\"][\"enum\"]\n        completion_text = self._match_items(completion_text, expected_items)\n\n    return {output_field_name: completion_text}\n</code></pre>"},{"location":"runtimes/#adala.runtimes._openai.OpenAIVisionRuntime","title":"<code>OpenAIVisionRuntime</code>","text":"<p>             Bases: <code>OpenAIChatRuntime</code></p> <p>Runtime that uses OpenAI API and vision models to perform the skill. Only compatible with OpenAI API version 1.0.0 or higher.</p> Source code in <code>adala/runtimes/_openai.py</code> <pre><code>class OpenAIVisionRuntime(OpenAIChatRuntime):\n    \"\"\"\n    Runtime that uses [OpenAI API](https://openai.com/) and vision models to perform the skill.\n    Only compatible with OpenAI API version 1.0.0 or higher.\n    \"\"\"\n\n    def record_to_record(\n        self,\n        record: Dict[str, str],\n        input_template: str,\n        instructions_template: str,\n        output_template: str,\n        extra_fields: Optional[Dict[str, str]] = None,\n        field_schema: Optional[Dict] = None,\n        instructions_first: bool = False,\n    ) -&gt; Dict[str, str]:\n        \"\"\"\n        Execute OpenAI request given record and templates for input, instructions and output.\n\n        Args:\n            record: Record to be used for input, instructions and output templates.\n            input_template: Template for input message.\n            instructions_template: Template for instructions message.\n            output_template: Template for output message.\n            extra_fields: Extra fields to be used in templates.\n            field_schema: Field jsonschema to be used for parsing templates.\n                         Field schema must contain \"format\": \"uri\" for image fields. For example:\n                            ```json\n                            {\n                                \"image\": {\n                                    \"type\": \"string\",\n                                    \"format\": \"uri\"\n                                }\n                            }\n                            ```\n            instructions_first: If True, instructions will be sent before input.\n        \"\"\"\n\n        if not check_if_new_openai_version():\n            raise NotImplementedError(\n                f\"{self.__class__.__name__} requires OpenAI API version 1.0.0 or higher.\"\n            )\n\n        extra_fields = extra_fields or {}\n        field_schema = field_schema or {}\n\n        output_fields = parse_template(\n            partial_str_format(output_template, **extra_fields), include_texts=False\n        )\n\n        if len(output_fields) &gt; 1:\n            raise NotImplementedError(\n                f\"{self.__class__.__name__} does not support multiple output fields. \"\n                f\"Found: {output_fields}\"\n            )\n        output_field = output_fields[0]\n        output_field_name = output_field[\"text\"]\n\n        input_fields = parse_template(input_template)\n\n        # split input template into text and image parts\n        input_text = \"\"\n        content = [\n            {\n                \"type\": \"text\",\n                \"text\": instructions_template,\n            }\n        ]\n        for field in input_fields:\n            if field[\"type\"] == \"text\":\n                input_text += field[\"text\"]\n            elif field[\"type\"] == \"var\":\n                if field[\"text\"] not in field_schema:\n                    input_text += record[field[\"text\"]]\n                elif field_schema[field[\"text\"]][\"type\"] == \"string\":\n                    if field_schema[field[\"text\"]].get(\"format\") == \"uri\":\n                        if input_text:\n                            content.append({\"type\": \"text\", \"text\": input_text})\n                            input_text = \"\"\n                        content.append(\n                            {\"type\": \"image_url\", \"image_url\": record[field[\"text\"]]}\n                        )\n                    else:\n                        input_text += record[field[\"text\"]]\n                else:\n                    raise ValueError(\n                        f'Unsupported field type: {field_schema[field[\"text\"]][\"type\"]}'\n                    )\n        if input_text:\n            content.append({\"type\": \"text\", \"text\": input_text})\n\n        if self.verbose:\n            print(f\"**Prompt content**:\\n{content}\")\n\n        completion = self._client.chat.completions.create(\n            model=self.openai_model,\n            messages=[{\"role\": \"user\", \"content\": content}],\n            max_tokens=self.max_tokens,\n        )\n\n        completion_text = completion.choices[0].message.content\n        return {output_field_name: completion_text}\n</code></pre>"},{"location":"runtimes/#adala.runtimes._openai.OpenAIVisionRuntime.record_to_record","title":"<code>record_to_record(record, input_template, instructions_template, output_template, extra_fields=None, field_schema=None, instructions_first=False)</code>","text":"<p>Execute OpenAI request given record and templates for input, instructions and output.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>Dict[str, str]</code> <p>Record to be used for input, instructions and output templates.</p> required <code>input_template</code> <code>str</code> <p>Template for input message.</p> required <code>instructions_template</code> <code>str</code> <p>Template for instructions message.</p> required <code>output_template</code> <code>str</code> <p>Template for output message.</p> required <code>extra_fields</code> <code>Optional[Dict[str, str]]</code> <p>Extra fields to be used in templates.</p> <code>None</code> <code>field_schema</code> <code>Optional[Dict]</code> <p>Field jsonschema to be used for parsing templates.          Field schema must contain \"format\": \"uri\" for image fields. For example:             <code>json             {                 \"image\": {                     \"type\": \"string\",                     \"format\": \"uri\"                 }             }</code></p> <code>None</code> <code>instructions_first</code> <code>bool</code> <p>If True, instructions will be sent before input.</p> <code>False</code> Source code in <code>adala/runtimes/_openai.py</code> <pre><code>def record_to_record(\n    self,\n    record: Dict[str, str],\n    input_template: str,\n    instructions_template: str,\n    output_template: str,\n    extra_fields: Optional[Dict[str, str]] = None,\n    field_schema: Optional[Dict] = None,\n    instructions_first: bool = False,\n) -&gt; Dict[str, str]:\n    \"\"\"\n    Execute OpenAI request given record and templates for input, instructions and output.\n\n    Args:\n        record: Record to be used for input, instructions and output templates.\n        input_template: Template for input message.\n        instructions_template: Template for instructions message.\n        output_template: Template for output message.\n        extra_fields: Extra fields to be used in templates.\n        field_schema: Field jsonschema to be used for parsing templates.\n                     Field schema must contain \"format\": \"uri\" for image fields. For example:\n                        ```json\n                        {\n                            \"image\": {\n                                \"type\": \"string\",\n                                \"format\": \"uri\"\n                            }\n                        }\n                        ```\n        instructions_first: If True, instructions will be sent before input.\n    \"\"\"\n\n    if not check_if_new_openai_version():\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} requires OpenAI API version 1.0.0 or higher.\"\n        )\n\n    extra_fields = extra_fields or {}\n    field_schema = field_schema or {}\n\n    output_fields = parse_template(\n        partial_str_format(output_template, **extra_fields), include_texts=False\n    )\n\n    if len(output_fields) &gt; 1:\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} does not support multiple output fields. \"\n            f\"Found: {output_fields}\"\n        )\n    output_field = output_fields[0]\n    output_field_name = output_field[\"text\"]\n\n    input_fields = parse_template(input_template)\n\n    # split input template into text and image parts\n    input_text = \"\"\n    content = [\n        {\n            \"type\": \"text\",\n            \"text\": instructions_template,\n        }\n    ]\n    for field in input_fields:\n        if field[\"type\"] == \"text\":\n            input_text += field[\"text\"]\n        elif field[\"type\"] == \"var\":\n            if field[\"text\"] not in field_schema:\n                input_text += record[field[\"text\"]]\n            elif field_schema[field[\"text\"]][\"type\"] == \"string\":\n                if field_schema[field[\"text\"]].get(\"format\") == \"uri\":\n                    if input_text:\n                        content.append({\"type\": \"text\", \"text\": input_text})\n                        input_text = \"\"\n                    content.append(\n                        {\"type\": \"image_url\", \"image_url\": record[field[\"text\"]]}\n                    )\n                else:\n                    input_text += record[field[\"text\"]]\n            else:\n                raise ValueError(\n                    f'Unsupported field type: {field_schema[field[\"text\"]][\"type\"]}'\n                )\n    if input_text:\n        content.append({\"type\": \"text\", \"text\": input_text})\n\n    if self.verbose:\n        print(f\"**Prompt content**:\\n{content}\")\n\n    completion = self._client.chat.completions.create(\n        model=self.openai_model,\n        messages=[{\"role\": \"user\", \"content\": content}],\n        max_tokens=self.max_tokens,\n    )\n\n    completion_text = completion.choices[0].message.content\n    return {output_field_name: completion_text}\n</code></pre>"},{"location":"runtimes/#adala.runtimes._guidance.GuidanceModelType","title":"<code>GuidanceModelType</code>","text":"<p>             Bases: <code>Enum</code></p> <p>Enumeration for LLM runtime model types.</p> Source code in <code>adala/runtimes/_guidance.py</code> <pre><code>class GuidanceModelType(enum.Enum):\n    \"\"\"Enumeration for LLM runtime model types.\"\"\"\n\n    OpenAI = \"OpenAI\"\n    Transformers = \"Transformers\"\n</code></pre>"},{"location":"runtimes/#adala.runtimes._guidance.GuidanceRuntime","title":"<code>GuidanceRuntime</code>","text":"<p>             Bases: <code>Runtime</code></p> <p>Runtime for LLMs powered by guidance. Here you can rely on constrained generation problem formulation, which require structured programs to be defined on the input and output data.</p> Source code in <code>adala/runtimes/_guidance.py</code> <pre><code>class GuidanceRuntime(Runtime):\n    \"\"\"\n    Runtime for LLMs powered by [guidance](https://github.com/guidance-ai/guidance).\n    Here you can rely on constrained generation problem formulation,\n    which require structured programs to be defined on the input and output data.\n    \"\"\"\n\n    llm_runtime_model_type: GuidanceModelType = GuidanceModelType.OpenAI\n    llm_params: Dict[str, str] = {\n        \"model\": \"gpt-3.5-turbo-instruct\",\n        # 'max_tokens': 10,\n        \"temperature\": 0,\n    }\n\n    _llm = None\n    # _program = None\n    #     # do not override this template\n    #     _llm_template: str = '''\\\n    # {{&gt;instructions_program}}\n    #\n    # {{&gt;input_program}}\n    # {{&gt;output_program}}'''\n\n    # do not override this template\n    _llm_templates: Dict[str, str] = {\n        True: \"\"\"\\\n{{&gt;instructions_program}}\n{{&gt;input_program}}\n{{&gt;output_program}}\"\"\",\n        False: \"\"\"\\\n{{&gt;input_program}}\n{{&gt;instructions_program}}\n{{&gt;output_program}}\"\"\",\n    }\n\n    def init_runtime(self) -&gt; Runtime:\n        \"\"\"\n        Initializes the runtime.\n        \"\"\"\n\n        # create an LLM instance\n        if self.llm_runtime_model_type.value == GuidanceModelType.OpenAI.value:\n            self._llm = guidance.llms.OpenAI(**self.llm_params)\n        elif self.llm_runtime_model_type.value == GuidanceModelType.Transformers.value:\n            self._llm = guidance.llms.Transformers(**self.llm_params)\n        else:\n            raise NotImplementedError(\n                f\"LLM runtime type {self.llm_runtime_model_type} is not implemented.\"\n            )\n        # self._program = guidance(self._llm_templates[self.instruction_first], llm=self._llm, silent=not self.verbose)\n        return self\n\n    def _input_template_to_guidance(self, input_template, program_input):\n        # TODO: this check is brittle, will likely to fail in various cases\n        # exclude guidance parameter from input\n        if \"text\" in program_input:\n            program_input[\"text_\"] = program_input[\"text\"]\n            del program_input[\"text\"]\n        if \"{text}\" in input_template:\n            input_template = input_template.replace(\"{text}\", \"{text_}\")\n\n        fields = parse_template(input_template, include_texts=False)\n        # replace {field_name} with {{field_name}}\n        for input_field in fields:\n            field_name = input_field[\"text\"]\n            if field_name in program_input:\n                input_template = input_template.replace(\n                    f\"{{{field_name}}}\", f\"{{{{{field_name}}}}}\"\n                )\n        return input_template\n\n    def _output_template_to_guidance(\n        self, output_template, program_input, output_fields, field_schema\n    ):\n        for output_field in output_fields:\n            field_name = output_field[\"text\"]\n            if (\n                field_name in field_schema\n                and field_schema[field_name][\"type\"] == \"array\"\n            ):\n                # when runtime is called with a categorical field:\n                #    runtime.record_to_record(\n                #        ...,\n                #        output_template='Predictions: {labels}',\n                #        field_schema={'labels': {'type': 'array', 'items': {'type': 'string', 'enum': ['a', 'b', 'c']}}}\n                #    )\n                # replace {field_name} with {select 'field_name' options=field_name_options}\n                # and add \"field_name_options\" to program input\n                program_input[f\"{field_name}_options\"] = field_schema[field_name][\n                    \"items\"\n                ][\"enum\"]\n                output_template = output_template.replace(\n                    f\"{{{field_name}}}\",\n                    f\"{{{{select '{field_name}' options={field_name}_options}}}}\",\n                )\n            else:\n                # In simple generation scenario, replace {field_name} with {{gen 'field_name'}}\n                output_template = output_template.replace(\n                    f\"{{{field_name}}}\", f\"{{{{gen '{field_name}'}}}}\"\n                )\n        return output_template\n\n    def record_to_record(\n        self,\n        record: Dict[str, str],\n        input_template: str,\n        instructions_template: str,\n        output_template: str,\n        extra_fields: Optional[Dict[str, Any]] = None,\n        field_schema: Optional[Dict] = None,\n        instructions_first: bool = True,\n    ) -&gt; Dict[str, str]:\n        \"\"\"\n        Generates a record from a record.\n\n        Args:\n            record (Dict[str, str]): Input record.\n            input_template (str): Input template.\n            instructions_template (str): Instructions template.\n            output_template (str): Output template.\n            extra_fields (Optional[Dict[str, Any]], optional): Extra fields. Defaults to None.\n            field_schema (Optional[Dict], optional): Field schema. Defaults to None.\n            instructions_first (bool, optional): Whether to put instructions first. Defaults to True.\n\n        Returns:\n            Dict[str, str]: Generated record.\n        \"\"\"\n\n        extra_fields = extra_fields or {}\n        field_schema = field_schema or {}\n\n        if not isinstance(record, dict):\n            record = record.to_dict()\n        else:\n            record = record.copy()\n        program_input = record\n        program_input.update(extra_fields)\n\n        output_fields = parse_template(\n            partial_str_format(output_template, **extra_fields), include_texts=False\n        )\n\n        input_template = self._input_template_to_guidance(input_template, program_input)\n        instructions_template = self._input_template_to_guidance(\n            instructions_template, program_input\n        )\n        output_template = self._output_template_to_guidance(\n            output_template, program_input, output_fields, field_schema\n        )\n\n        program_input[\"input_program\"] = guidance(input_template, llm=self._llm)\n        program_input[\"instructions_program\"] = guidance(\n            instructions_template, llm=self._llm\n        )\n        program_input[\"output_program\"] = guidance(output_template, llm=self._llm)\n\n        if self.verbose:\n            print(program_input)\n\n        program = guidance(\n            self._llm_templates[instructions_first],\n            llm=self._llm,\n            silent=not self.verbose,\n        )\n\n        result = program(silent=not self.verbose, **program_input)\n\n        output = {}\n        for output_field in output_fields:\n            if output_field[\"text\"] in extra_fields:\n                continue\n            if output_field[\"text\"] not in result:\n                raise ValueError(\n                    f'Output field {output_field[\"text\"]} is not in the output. '\n                    f\"The current output is {result}.\"\n                )\n            output[output_field[\"text\"]] = result[output_field[\"text\"]].strip()\n        return output\n</code></pre>"},{"location":"runtimes/#adala.runtimes._guidance.GuidanceRuntime.init_runtime","title":"<code>init_runtime()</code>","text":"<p>Initializes the runtime.</p> Source code in <code>adala/runtimes/_guidance.py</code> <pre><code>def init_runtime(self) -&gt; Runtime:\n    \"\"\"\n    Initializes the runtime.\n    \"\"\"\n\n    # create an LLM instance\n    if self.llm_runtime_model_type.value == GuidanceModelType.OpenAI.value:\n        self._llm = guidance.llms.OpenAI(**self.llm_params)\n    elif self.llm_runtime_model_type.value == GuidanceModelType.Transformers.value:\n        self._llm = guidance.llms.Transformers(**self.llm_params)\n    else:\n        raise NotImplementedError(\n            f\"LLM runtime type {self.llm_runtime_model_type} is not implemented.\"\n        )\n    # self._program = guidance(self._llm_templates[self.instruction_first], llm=self._llm, silent=not self.verbose)\n    return self\n</code></pre>"},{"location":"runtimes/#adala.runtimes._guidance.GuidanceRuntime.record_to_record","title":"<code>record_to_record(record, input_template, instructions_template, output_template, extra_fields=None, field_schema=None, instructions_first=True)</code>","text":"<p>Generates a record from a record.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>Dict[str, str]</code> <p>Input record.</p> required <code>input_template</code> <code>str</code> <p>Input template.</p> required <code>instructions_template</code> <code>str</code> <p>Instructions template.</p> required <code>output_template</code> <code>str</code> <p>Output template.</p> required <code>extra_fields</code> <code>Optional[Dict[str, Any]]</code> <p>Extra fields. Defaults to None.</p> <code>None</code> <code>field_schema</code> <code>Optional[Dict]</code> <p>Field schema. Defaults to None.</p> <code>None</code> <code>instructions_first</code> <code>bool</code> <p>Whether to put instructions first. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dict[str, str]: Generated record.</p> Source code in <code>adala/runtimes/_guidance.py</code> <pre><code>def record_to_record(\n    self,\n    record: Dict[str, str],\n    input_template: str,\n    instructions_template: str,\n    output_template: str,\n    extra_fields: Optional[Dict[str, Any]] = None,\n    field_schema: Optional[Dict] = None,\n    instructions_first: bool = True,\n) -&gt; Dict[str, str]:\n    \"\"\"\n    Generates a record from a record.\n\n    Args:\n        record (Dict[str, str]): Input record.\n        input_template (str): Input template.\n        instructions_template (str): Instructions template.\n        output_template (str): Output template.\n        extra_fields (Optional[Dict[str, Any]], optional): Extra fields. Defaults to None.\n        field_schema (Optional[Dict], optional): Field schema. Defaults to None.\n        instructions_first (bool, optional): Whether to put instructions first. Defaults to True.\n\n    Returns:\n        Dict[str, str]: Generated record.\n    \"\"\"\n\n    extra_fields = extra_fields or {}\n    field_schema = field_schema or {}\n\n    if not isinstance(record, dict):\n        record = record.to_dict()\n    else:\n        record = record.copy()\n    program_input = record\n    program_input.update(extra_fields)\n\n    output_fields = parse_template(\n        partial_str_format(output_template, **extra_fields), include_texts=False\n    )\n\n    input_template = self._input_template_to_guidance(input_template, program_input)\n    instructions_template = self._input_template_to_guidance(\n        instructions_template, program_input\n    )\n    output_template = self._output_template_to_guidance(\n        output_template, program_input, output_fields, field_schema\n    )\n\n    program_input[\"input_program\"] = guidance(input_template, llm=self._llm)\n    program_input[\"instructions_program\"] = guidance(\n        instructions_template, llm=self._llm\n    )\n    program_input[\"output_program\"] = guidance(output_template, llm=self._llm)\n\n    if self.verbose:\n        print(program_input)\n\n    program = guidance(\n        self._llm_templates[instructions_first],\n        llm=self._llm,\n        silent=not self.verbose,\n    )\n\n    result = program(silent=not self.verbose, **program_input)\n\n    output = {}\n    for output_field in output_fields:\n        if output_field[\"text\"] in extra_fields:\n            continue\n        if output_field[\"text\"] not in result:\n            raise ValueError(\n                f'Output field {output_field[\"text\"]} is not in the output. '\n                f\"The current output is {result}.\"\n            )\n        output[output_field[\"text\"]] = result[output_field[\"text\"]].strip()\n    return output\n</code></pre>"},{"location":"runtimes/#adala.runtimes._langchain.LangChainRuntime","title":"<code>LangChainRuntime</code>","text":"<p>             Bases: <code>Runtime</code></p> <p>Runtime that uses LangChain models to perform the skill.</p> Source code in <code>adala/runtimes/_langchain.py</code> <pre><code>class LangChainRuntime(Runtime):\n    \"\"\"\n    Runtime that uses [LangChain](https://www.langchain.com/) models to perform the skill.\n    \"\"\"\n\n    lc_model_name: str = Field(alias=\"model\")\n\n    def _prepare_chain(\n        self,\n        record: Dict[str, str],\n        input_template: str,\n        instructions_template: str,\n        output_template: str,\n        extra_fields: Optional[Dict[str, Any]] = None,\n        field_schema: Optional[Dict] = None,\n    ):\n        field_schema = field_schema or {}\n        extra_fields = extra_fields or {}\n        output_fields = parse_template(\n            partial_str_format(output_template, **record, **extra_fields),\n            include_texts=False,\n        )\n        response_schemas = []\n        for output_field in output_fields:\n            name = output_field[\"text\"]\n            if name in field_schema and \"description\" in field_schema[name]:\n                description = field_schema[name][\"description\"]\n            else:\n                description = name\n            response_schemas.append(ResponseSchema(name=name, description=description))\n\n        output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n        format_instructions = output_parser.get_format_instructions()\n\n        model = ChatOpenAI(model_name=self.lc_model_name, verbose=self.verbose)\n\n        prompt = ChatPromptTemplate.from_template(\n            \"{instructions_template}\\n{format_instructions}\\n{input_template}\",\n            partial_variables={\n                \"format_instructions\": format_instructions,\n                \"instructions_template\": instructions_template.format(\n                    **record, **extra_fields\n                ),\n                \"input_template\": input_template.format(**record, **extra_fields),\n            },\n        )\n\n        if self.verbose:\n            print(f\"**Prompt content**:\\n{prompt}\")\n\n        chain = prompt | model | output_parser\n        return chain\n\n    def record_to_record(\n        self,\n        record: Dict[str, str],\n        input_template: str,\n        instructions_template: str,\n        output_template: str,\n        extra_fields: Optional[Dict[str, Any]] = None,\n        field_schema: Optional[Dict] = None,\n    ) -&gt; Dict[str, str]:\n        chain = self._prepare_chain(\n            record,\n            input_template,\n            instructions_template,\n            output_template,\n            extra_fields,\n            field_schema,\n        )\n        return chain.invoke(record)\n</code></pre>"},{"location":"skills/","title":"Skills","text":""},{"location":"skills/#adala.skills._base.AnalysisSkill","title":"<code>AnalysisSkill</code>","text":"<p>             Bases: <code>Skill</code></p> <p>Analysis skill that analyzes a dataframe and returns a record (e.g. for data analysis purposes). See base class Skill for more information about the attributes.</p> Source code in <code>adala/skills/_base.py</code> <pre><code>class AnalysisSkill(Skill):\n    \"\"\"\n    Analysis skill that analyzes a dataframe and returns a record (e.g. for data analysis purposes).\n    See base class Skill for more information about the attributes.\n    \"\"\"\n\n    def apply(\n        self,\n        input: Union[InternalDataFrame, InternalSeries, Dict],\n        runtime: Runtime,\n    ) -&gt; InternalSeries:\n        \"\"\"\n        Applies the skill to a dataframe and returns a record.\n\n        Args:\n            input (InternalDataFrame): The input data to be processed.\n            runtime (Runtime): The runtime instance to be used for processing.\n\n        Returns:\n            InternalSeries: The record containing the analysis results.\n        \"\"\"\n        if isinstance(input, InternalSeries):\n            input = input.to_frame()\n        elif isinstance(input, dict):\n            input = InternalDataFrame([input])\n\n        extra_fields = self._get_extra_fields()\n\n        aggregated_input = input.apply(\n            lambda row: self.input_template.format(**row, **extra_fields), axis=1\n        ).str.cat(sep=\"\\n\")\n\n        output = runtime.record_to_record(\n            {\"input\": aggregated_input},\n            input_template=\"{input}\",\n            output_template=self.output_template,\n            instructions_template=self.instructions,\n            field_schema=self.field_schema,\n            extra_fields=self._get_extra_fields(),\n            instructions_first=self.instructions_first,\n        )\n        # output['input'] = aggregated_input\n        # concatenate input and output and return dataframe\n        return InternalSeries(output)\n\n    def improve(self, **kwargs):\n        \"\"\"\n        Improves the skill.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"skills/#adala.skills._base.AnalysisSkill.apply","title":"<code>apply(input, runtime)</code>","text":"<p>Applies the skill to a dataframe and returns a record.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>InternalDataFrame</code> <p>The input data to be processed.</p> required <code>runtime</code> <code>Runtime</code> <p>The runtime instance to be used for processing.</p> required <p>Returns:</p> Name Type Description <code>InternalSeries</code> <code>InternalSeries</code> <p>The record containing the analysis results.</p> Source code in <code>adala/skills/_base.py</code> <pre><code>def apply(\n    self,\n    input: Union[InternalDataFrame, InternalSeries, Dict],\n    runtime: Runtime,\n) -&gt; InternalSeries:\n    \"\"\"\n    Applies the skill to a dataframe and returns a record.\n\n    Args:\n        input (InternalDataFrame): The input data to be processed.\n        runtime (Runtime): The runtime instance to be used for processing.\n\n    Returns:\n        InternalSeries: The record containing the analysis results.\n    \"\"\"\n    if isinstance(input, InternalSeries):\n        input = input.to_frame()\n    elif isinstance(input, dict):\n        input = InternalDataFrame([input])\n\n    extra_fields = self._get_extra_fields()\n\n    aggregated_input = input.apply(\n        lambda row: self.input_template.format(**row, **extra_fields), axis=1\n    ).str.cat(sep=\"\\n\")\n\n    output = runtime.record_to_record(\n        {\"input\": aggregated_input},\n        input_template=\"{input}\",\n        output_template=self.output_template,\n        instructions_template=self.instructions,\n        field_schema=self.field_schema,\n        extra_fields=self._get_extra_fields(),\n        instructions_first=self.instructions_first,\n    )\n    # output['input'] = aggregated_input\n    # concatenate input and output and return dataframe\n    return InternalSeries(output)\n</code></pre>"},{"location":"skills/#adala.skills._base.AnalysisSkill.improve","title":"<code>improve(**kwargs)</code>","text":"<p>Improves the skill.</p> Source code in <code>adala/skills/_base.py</code> <pre><code>def improve(self, **kwargs):\n    \"\"\"\n    Improves the skill.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"skills/#adala.skills._base.Skill","title":"<code>Skill</code>","text":"<p>             Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Abstract base class representing a skill.</p> <p>Provides methods to interact with and obtain information about skills.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Unique name of the skill.</p> <code>instructions</code> <code>str</code> <p>Instructs agent what to do with the input data.</p> <code>input_template</code> <code>str</code> <p>Template for the input data.</p> <code>output_template</code> <code>str</code> <p>Template for the output data.</p> <code>description</code> <code>Optional[str]</code> <p>Description of the skill.</p> <code>field_schema</code> <code>Optional[Dict]</code> <p>Field JSON schema to use in the templates. Defaults to all fields are strings, i.e. analogous to {\"field_n\": {\"type\": \"string\"}}.</p> <code>extra_fields</code> <code>Optional[Dict[str, str]]</code> <p>Extra fields to use in the templates. Defaults to None.</p> <code>instructions_first</code> <code>bool</code> <p>Flag indicating if instructions should be executed before input. Defaults to True.</p> <code>verbose</code> <code>bool</code> <p>Flag indicating if runtime outputs should be verbose. Defaults to False.</p> Source code in <code>adala/skills/_base.py</code> <pre><code>class Skill(BaseModel, ABC):\n    \"\"\"\n    Abstract base class representing a skill.\n\n    Provides methods to interact with and obtain information about skills.\n\n    Attributes:\n        name (str): Unique name of the skill.\n        instructions (str): Instructs agent what to do with the input data.\n        input_template (str): Template for the input data.\n        output_template (str): Template for the output data.\n        description (Optional[str]): Description of the skill.\n        field_schema (Optional[Dict]): Field [JSON schema](https://json-schema.org/) to use in the templates. Defaults to all fields are strings,\n            i.e. analogous to {\"field_n\": {\"type\": \"string\"}}.\n        extra_fields (Optional[Dict[str, str]]): Extra fields to use in the templates. Defaults to None.\n        instructions_first (bool): Flag indicating if instructions should be executed before input. Defaults to True.\n        verbose (bool): Flag indicating if runtime outputs should be verbose. Defaults to False.\n    \"\"\"\n\n    name: str = Field(\n        title=\"Skill name\",\n        description=\"Unique name of the skill\",\n        examples=[\"labeling\", \"classification\", \"text-generation\"],\n    )\n    instructions: str = Field(\n        title=\"Skill instructions\",\n        description=\"Instructs agent what to do with the input data. \"\n        \"Can use templating to refer to input fields.\",\n        examples=[\"Label the input text with the following labels: {labels}\"],\n    )\n    input_template: str = Field(\n        title=\"Input template\",\n        description=\"Template for the input data. \"\n        \"Can use templating to refer to input parameters and perform data transformations.\",\n        examples=[\"Input: {input}\", \"Input: {input}\\nLabels: {labels}\\nOutput: \"],\n    )\n    output_template: str = Field(\n        title=\"Output template\",\n        description=\"Template for the output data. \"\n        \"Can use templating to refer to input parameters and perform data transformations\",\n        examples=[\"Output: {output}\", \"{predictions}\"],\n    )\n    description: Optional[str] = Field(\n        default=\"\",\n        title=\"Skill description\",\n        description=\"Description of the skill. Can be used to retrieve skill from the library.\",\n        examples=[\"The skill to perform sentiment analysis on the input text.\"],\n    )\n    field_schema: Optional[Dict[str, Any]] = Field(\n        default=None,\n        title=\"Field schema\",\n        description=\"JSON schema for the fields of the input and output data.\",\n        examples=[\n            {\n                \"input\": {\"type\": \"string\"},\n                \"output\": {\"type\": \"string\"},\n                \"labels\": {\n                    \"type\": \"array\",\n                    \"items\": {\n                        \"type\": \"string\",\n                        \"enum\": [\"positive\", \"negative\", \"neutral\"],\n                    },\n                },\n            }\n        ],\n    )\n    instructions_first: bool = Field(\n        default=True,\n        title=\"Instructions first\",\n        description=\"Flag indicating if instructions should be shown before the input data.\",\n        examples=[True, False],\n    )\n\n    frozen: bool = Field(\n        default=False,\n        title=\"Frozen\",\n        description=\"Flag indicating if the skill is frozen.\",\n        examples=[True, False],\n    )\n\n    def _get_extra_fields(self):\n        \"\"\"\n        Retrieves fields that are not categorized as system fields.\n\n        Returns:\n            dict: A dictionary containing fields that are not system fields.\n        \"\"\"\n\n        # TODO: more robust way to exclude system fields\n        system_fields = {\n            \"name\",\n            \"description\",\n            \"input_template\",\n            \"output_template\",\n            \"instructions\",\n            \"field_schema\",\n        }\n        extra_fields = self.model_dump(exclude=system_fields)\n        return extra_fields\n\n    def get_output_fields(self):\n        \"\"\"\n        Retrieves output fields.\n\n        Returns:\n            List[str]: A list of output fields.\n        \"\"\"\n        extra_fields = self._get_extra_fields()\n        # TODO: input fields are not considered - shall we disallow input fields in output template?\n        output_fields = parse_template(\n            partial_str_format(self.output_template, **extra_fields),\n            include_texts=False,\n        )\n        return [f[\"text\"] for f in output_fields]\n\n    @abstractmethod\n    def apply(self, input, runtime):\n        \"\"\"\n        Base method for applying the skill.\n        \"\"\"\n\n    @abstractmethod\n    def improve(self, predictions, train_skill_output, feedback, runtime):\n        \"\"\"\n        Base method for improving the skill.\n        \"\"\"\n</code></pre>"},{"location":"skills/#adala.skills._base.Skill.apply","title":"<code>apply(input, runtime)</code>  <code>abstractmethod</code>","text":"<p>Base method for applying the skill.</p> Source code in <code>adala/skills/_base.py</code> <pre><code>@abstractmethod\ndef apply(self, input, runtime):\n    \"\"\"\n    Base method for applying the skill.\n    \"\"\"\n</code></pre>"},{"location":"skills/#adala.skills._base.Skill.get_output_fields","title":"<code>get_output_fields()</code>","text":"<p>Retrieves output fields.</p> <p>Returns:</p> Type Description <p>List[str]: A list of output fields.</p> Source code in <code>adala/skills/_base.py</code> <pre><code>def get_output_fields(self):\n    \"\"\"\n    Retrieves output fields.\n\n    Returns:\n        List[str]: A list of output fields.\n    \"\"\"\n    extra_fields = self._get_extra_fields()\n    # TODO: input fields are not considered - shall we disallow input fields in output template?\n    output_fields = parse_template(\n        partial_str_format(self.output_template, **extra_fields),\n        include_texts=False,\n    )\n    return [f[\"text\"] for f in output_fields]\n</code></pre>"},{"location":"skills/#adala.skills._base.Skill.improve","title":"<code>improve(predictions, train_skill_output, feedback, runtime)</code>  <code>abstractmethod</code>","text":"<p>Base method for improving the skill.</p> Source code in <code>adala/skills/_base.py</code> <pre><code>@abstractmethod\ndef improve(self, predictions, train_skill_output, feedback, runtime):\n    \"\"\"\n    Base method for improving the skill.\n    \"\"\"\n</code></pre>"},{"location":"skills/#adala.skills._base.SynthesisSkill","title":"<code>SynthesisSkill</code>","text":"<p>             Bases: <code>Skill</code></p> <p>Synthesis skill that synthesize a dataframe from a record (e.g. for dataset generation purposes). See base class Skill for more information about the attributes.</p> Source code in <code>adala/skills/_base.py</code> <pre><code>class SynthesisSkill(Skill):\n    \"\"\"\n    Synthesis skill that synthesize a dataframe from a record (e.g. for dataset generation purposes).\n    See base class Skill for more information about the attributes.\n    \"\"\"\n\n    def apply(\n        self,\n        input: Union[Dict, InternalSeries],\n        runtime: Runtime,\n    ) -&gt; InternalDataFrame:\n        \"\"\"\n        Applies the skill to a record and returns a dataframe.\n\n        Args:\n            input (InternalSeries): The input data to be processed.\n            runtime (Runtime): The runtime instance to be used for processing.\n\n        Returns:\n            InternalDataFrame: The synthesized data.\n        \"\"\"\n        if isinstance(input, InternalSeries):\n            input = input.to_dict()\n        return runtime.record_to_batch(\n            input,\n            input_template=self.input_template,\n            output_template=self.output_template,\n            instructions_template=self.instructions,\n            field_schema=self.field_schema,\n            extra_fields=self._get_extra_fields(),\n            instructions_first=self.instructions_first,\n        )\n\n    def improve(self, **kwargs):\n        \"\"\"\n        Improves the skill.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"skills/#adala.skills._base.SynthesisSkill.apply","title":"<code>apply(input, runtime)</code>","text":"<p>Applies the skill to a record and returns a dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>InternalSeries</code> <p>The input data to be processed.</p> required <code>runtime</code> <code>Runtime</code> <p>The runtime instance to be used for processing.</p> required <p>Returns:</p> Name Type Description <code>InternalDataFrame</code> <code>InternalDataFrame</code> <p>The synthesized data.</p> Source code in <code>adala/skills/_base.py</code> <pre><code>def apply(\n    self,\n    input: Union[Dict, InternalSeries],\n    runtime: Runtime,\n) -&gt; InternalDataFrame:\n    \"\"\"\n    Applies the skill to a record and returns a dataframe.\n\n    Args:\n        input (InternalSeries): The input data to be processed.\n        runtime (Runtime): The runtime instance to be used for processing.\n\n    Returns:\n        InternalDataFrame: The synthesized data.\n    \"\"\"\n    if isinstance(input, InternalSeries):\n        input = input.to_dict()\n    return runtime.record_to_batch(\n        input,\n        input_template=self.input_template,\n        output_template=self.output_template,\n        instructions_template=self.instructions,\n        field_schema=self.field_schema,\n        extra_fields=self._get_extra_fields(),\n        instructions_first=self.instructions_first,\n    )\n</code></pre>"},{"location":"skills/#adala.skills._base.SynthesisSkill.improve","title":"<code>improve(**kwargs)</code>","text":"<p>Improves the skill.</p> Source code in <code>adala/skills/_base.py</code> <pre><code>def improve(self, **kwargs):\n    \"\"\"\n    Improves the skill.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"skills/#adala.skills._base.TransformSkill","title":"<code>TransformSkill</code>","text":"<p>             Bases: <code>Skill</code></p> <p>Transform skill that transforms a dataframe to another dataframe (e.g. for data annotation purposes). See base class Skill for more information about the attributes.</p> Source code in <code>adala/skills/_base.py</code> <pre><code>class TransformSkill(Skill):\n    \"\"\"\n    Transform skill that transforms a dataframe to another dataframe (e.g. for data annotation purposes).\n    See base class Skill for more information about the attributes.\n    \"\"\"\n\n    def apply(\n        self,\n        input: InternalDataFrame,\n        runtime: Runtime,\n    ) -&gt; InternalDataFrame:\n        \"\"\"\n        Applies the skill to a dataframe and returns another dataframe.\n\n        Args:\n            input (InternalDataFrame): The input data to be processed.\n            runtime (Runtime): The runtime instance to be used for processing.\n\n        Returns:\n            InternalDataFrame: The transformed data.\n        \"\"\"\n\n        return runtime.batch_to_batch(\n            input,\n            input_template=self.input_template,\n            output_template=self.output_template,\n            instructions_template=self.instructions,\n            field_schema=self.field_schema,\n            extra_fields=self._get_extra_fields(),\n            instructions_first=self.instructions_first,\n        )\n\n    def improve(\n        self,\n        predictions: InternalDataFrame,\n        train_skill_output: str,\n        feedback,\n        runtime: Runtime,\n    ):\n        \"\"\"\n        Improves the skill.\n\n        Args:\n            predictions (InternalDataFrame): The predictions made by the skill.\n            train_skill_output (str): The name of the output field of the skill.\n            feedback (InternalDataFrame): The feedback provided by the user.\n            runtime (Runtime): The runtime instance to be used for processing (CURRENTLY SUPPORTS ONLY `OpenAIChatRuntime`).\n\n        \"\"\"\n        if (\n            feedback.match[train_skill_output].all()\n            and not feedback.match[train_skill_output].isna().all()\n        ):\n            # nothing to improve\n            return\n\n        fb = feedback.feedback.rename(\n            columns=lambda x: x + \"__fb\" if x in predictions.columns else x\n        )\n        analyzed_df = fb.merge(predictions, left_index=True, right_index=True)\n\n        examples = []\n\n        for i, row in enumerate(analyzed_df.to_dict(orient=\"records\")):\n            # if fb marked as NaN, skip\n            if not row[f\"{train_skill_output}__fb\"]:\n                continue\n            examples.append(\n                f\"### Example #{i}\\n\\n\"\n                f\"{self.input_template.format(**row)}\\n\\n\"\n                f\"{self.output_template.format(**row)}\\n\\n\"\n                f'User feedback: {row[f\"{train_skill_output}__fb\"]}\\n\\n'\n            )\n\n        examples = \"\\n\".join(examples)\n\n        messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n\n        # full template\n        if self.instructions_first:\n            full_template = f\"\"\"\n{{prompt}}\n{self.input_template}\n{self.output_template}\"\"\"\n        else:\n            full_template = f\"\"\"\n{self.input_template}\n{{prompt}}\n{self.output_template}\"\"\"\n\n        messages += [\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\nA prompt is a text paragraph that outlines the expected actions and instructs the large language model (LLM) to \\\ngenerate a specific output. This prompt is concatenated with the input text, and the \\\nmodel then creates the required output.\nThis describes the full template how the prompt is concatenated with the input to produce the output:\n\n```\n{full_template}\n```\n\nHere:\n- \"{self.input_template}\" is input template,\n- \"{{prompt}}\" is the LLM prompt,\n- \"{self.output_template}\" is the output template.\n\nModel can produce erroneous output if a prompt is not well defined. \\\nIn our collaboration, we\u2019ll work together to refine a prompt. The process consists of two main steps:\n\n## Step 1\nI will provide you with the current prompt along with prediction examples. Each example contains the input text, the final prediction produced by the model, and the user feedback. \\\nUser feedback indicates whether the model prediction is correct or not. \\\nYour task is to analyze the examples and user feedback, determining whether the \\\nexisting instruction is describing the task reflected by these examples precisely, and suggests changes to the prompt to address the incorrect predictions.\n\n## Step 2\nNext, you will carefully review your reasoning in step 1, integrate the insights to refine the prompt, \\\nand provide me with the new prompt that improves the model\u2019s performance.\"\"\",\n            }\n        ]\n\n        messages += [\n            {\n                \"role\": \"assistant\",\n                \"content\": \"Sure, I\u2019d be happy to help you with this prompt engineering problem. \"\n                \"Please provide me with the current prompt and the examples with user feedback.\",\n            }\n        ]\n\n        messages += [\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n## Current prompt\n{self.instructions}\n\n## Examples\n{examples}\n\nSummarize your analysis about incorrect predictions and suggest changes to the prompt.\"\"\",\n            }\n        ]\n\n        reasoning = runtime.execute(messages)\n\n        messages += [\n            {\"role\": \"assistant\", \"content\": reasoning},\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\nNow please carefully review your reasoning in Step 1 and help with Step 2: refining the prompt.\n\n## Current prompt\n{self.instructions}\n\n## Follow this guidance to refine the prompt:\n\n1. The new prompt should should describe the task precisely, and address the points raised in the user feedback.\n\n2. The new prompt should be similar to the current instruction, and only differ in the parts that address the issues you identified in Step 1.\n    Example:\n    - Current prompt: \"The model should generate a summary of the input text.\"\n    - New prompt: \"The model should generate a summary of the input text. Pay attention to the original style.\"\n\n3. Reply only with the new prompt. Do not include input and output templates in the prompt.\"\"\",\n            },\n        ]\n\n        # display dialogue:\n        for message in messages:\n            print(f'\"{{{message[\"role\"]}}}\":\\n{message[\"content\"]}')\n        new_prompt = runtime.execute(messages)\n        self.instructions = new_prompt\n</code></pre>"},{"location":"skills/#adala.skills._base.TransformSkill.apply","title":"<code>apply(input, runtime)</code>","text":"<p>Applies the skill to a dataframe and returns another dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>InternalDataFrame</code> <p>The input data to be processed.</p> required <code>runtime</code> <code>Runtime</code> <p>The runtime instance to be used for processing.</p> required <p>Returns:</p> Name Type Description <code>InternalDataFrame</code> <code>InternalDataFrame</code> <p>The transformed data.</p> Source code in <code>adala/skills/_base.py</code> <pre><code>def apply(\n    self,\n    input: InternalDataFrame,\n    runtime: Runtime,\n) -&gt; InternalDataFrame:\n    \"\"\"\n    Applies the skill to a dataframe and returns another dataframe.\n\n    Args:\n        input (InternalDataFrame): The input data to be processed.\n        runtime (Runtime): The runtime instance to be used for processing.\n\n    Returns:\n        InternalDataFrame: The transformed data.\n    \"\"\"\n\n    return runtime.batch_to_batch(\n        input,\n        input_template=self.input_template,\n        output_template=self.output_template,\n        instructions_template=self.instructions,\n        field_schema=self.field_schema,\n        extra_fields=self._get_extra_fields(),\n        instructions_first=self.instructions_first,\n    )\n</code></pre>"},{"location":"skills/#adala.skills._base.TransformSkill.improve","title":"<code>improve(predictions, train_skill_output, feedback, runtime)</code>","text":"<p>Improves the skill.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>InternalDataFrame</code> <p>The predictions made by the skill.</p> required <code>train_skill_output</code> <code>str</code> <p>The name of the output field of the skill.</p> required <code>feedback</code> <code>InternalDataFrame</code> <p>The feedback provided by the user.</p> required <code>runtime</code> <code>Runtime</code> <p>The runtime instance to be used for processing (CURRENTLY SUPPORTS ONLY <code>OpenAIChatRuntime</code>).</p> required Source code in <code>adala/skills/_base.py</code> <pre><code>    def improve(\n        self,\n        predictions: InternalDataFrame,\n        train_skill_output: str,\n        feedback,\n        runtime: Runtime,\n    ):\n        \"\"\"\n        Improves the skill.\n\n        Args:\n            predictions (InternalDataFrame): The predictions made by the skill.\n            train_skill_output (str): The name of the output field of the skill.\n            feedback (InternalDataFrame): The feedback provided by the user.\n            runtime (Runtime): The runtime instance to be used for processing (CURRENTLY SUPPORTS ONLY `OpenAIChatRuntime`).\n\n        \"\"\"\n        if (\n            feedback.match[train_skill_output].all()\n            and not feedback.match[train_skill_output].isna().all()\n        ):\n            # nothing to improve\n            return\n\n        fb = feedback.feedback.rename(\n            columns=lambda x: x + \"__fb\" if x in predictions.columns else x\n        )\n        analyzed_df = fb.merge(predictions, left_index=True, right_index=True)\n\n        examples = []\n\n        for i, row in enumerate(analyzed_df.to_dict(orient=\"records\")):\n            # if fb marked as NaN, skip\n            if not row[f\"{train_skill_output}__fb\"]:\n                continue\n            examples.append(\n                f\"### Example #{i}\\n\\n\"\n                f\"{self.input_template.format(**row)}\\n\\n\"\n                f\"{self.output_template.format(**row)}\\n\\n\"\n                f'User feedback: {row[f\"{train_skill_output}__fb\"]}\\n\\n'\n            )\n\n        examples = \"\\n\".join(examples)\n\n        messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n\n        # full template\n        if self.instructions_first:\n            full_template = f\"\"\"\n{{prompt}}\n{self.input_template}\n{self.output_template}\"\"\"\n        else:\n            full_template = f\"\"\"\n{self.input_template}\n{{prompt}}\n{self.output_template}\"\"\"\n\n        messages += [\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\nA prompt is a text paragraph that outlines the expected actions and instructs the large language model (LLM) to \\\ngenerate a specific output. This prompt is concatenated with the input text, and the \\\nmodel then creates the required output.\nThis describes the full template how the prompt is concatenated with the input to produce the output:\n\n```\n{full_template}\n```\n\nHere:\n- \"{self.input_template}\" is input template,\n- \"{{prompt}}\" is the LLM prompt,\n- \"{self.output_template}\" is the output template.\n\nModel can produce erroneous output if a prompt is not well defined. \\\nIn our collaboration, we\u2019ll work together to refine a prompt. The process consists of two main steps:\n\n## Step 1\nI will provide you with the current prompt along with prediction examples. Each example contains the input text, the final prediction produced by the model, and the user feedback. \\\nUser feedback indicates whether the model prediction is correct or not. \\\nYour task is to analyze the examples and user feedback, determining whether the \\\nexisting instruction is describing the task reflected by these examples precisely, and suggests changes to the prompt to address the incorrect predictions.\n\n## Step 2\nNext, you will carefully review your reasoning in step 1, integrate the insights to refine the prompt, \\\nand provide me with the new prompt that improves the model\u2019s performance.\"\"\",\n            }\n        ]\n\n        messages += [\n            {\n                \"role\": \"assistant\",\n                \"content\": \"Sure, I\u2019d be happy to help you with this prompt engineering problem. \"\n                \"Please provide me with the current prompt and the examples with user feedback.\",\n            }\n        ]\n\n        messages += [\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n## Current prompt\n{self.instructions}\n\n## Examples\n{examples}\n\nSummarize your analysis about incorrect predictions and suggest changes to the prompt.\"\"\",\n            }\n        ]\n\n        reasoning = runtime.execute(messages)\n\n        messages += [\n            {\"role\": \"assistant\", \"content\": reasoning},\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\nNow please carefully review your reasoning in Step 1 and help with Step 2: refining the prompt.\n\n## Current prompt\n{self.instructions}\n\n## Follow this guidance to refine the prompt:\n\n1. The new prompt should should describe the task precisely, and address the points raised in the user feedback.\n\n2. The new prompt should be similar to the current instruction, and only differ in the parts that address the issues you identified in Step 1.\n    Example:\n    - Current prompt: \"The model should generate a summary of the input text.\"\n    - New prompt: \"The model should generate a summary of the input text. Pay attention to the original style.\"\n\n3. Reply only with the new prompt. Do not include input and output templates in the prompt.\"\"\",\n            },\n        ]\n\n        # display dialogue:\n        for message in messages:\n            print(f'\"{{{message[\"role\"]}}}\":\\n{message[\"content\"]}')\n        new_prompt = runtime.execute(messages)\n        self.instructions = new_prompt\n</code></pre>"},{"location":"skills/#adala.skills.skillset.LinearSkillSet","title":"<code>LinearSkillSet</code>","text":"<p>             Bases: <code>SkillSet</code></p> <p>Represents a sequence of skills that are acquired in a specific order to achieve a goal.</p> <p>LinearSkillSet ensures that skills are applied in a sequential manner.</p> <p>Attributes:</p> Name Type Description <code>skills</code> <code>Union[List[Skill], Dict[str, Skill]]</code> <p>Provided skills</p> <code>skill_sequence</code> <code>List[str]</code> <p>Ordered list of skill names indicating the order                                   in which they should be acquired.</p> <p>Examples:</p> <pre><code>Create a LinearSkillSet with a list of skills specified as BaseSkill instances:\n&gt;&gt;&gt; from adala.skills import LinearSkillSet, TransformSkill, AnalysisSkill, ClassificationSkill\n&gt;&gt;&gt; skillset = LinearSkillSet(skills=[TransformSkill(), ClassificationSkill(), AnalysisSkill()])\n</code></pre> Source code in <code>adala/skills/skillset.py</code> <pre><code>class LinearSkillSet(SkillSet):\n    \"\"\"\n    Represents a sequence of skills that are acquired in a specific order to achieve a goal.\n\n    LinearSkillSet ensures that skills are applied in a sequential manner.\n\n    Attributes:\n        skills (Union[List[Skill], Dict[str, Skill]]): Provided skills\n        skill_sequence (List[str], optional): Ordered list of skill names indicating the order\n                                              in which they should be acquired.\n\n    Examples:\n\n        Create a LinearSkillSet with a list of skills specified as BaseSkill instances:\n        &gt;&gt;&gt; from adala.skills import LinearSkillSet, TransformSkill, AnalysisSkill, ClassificationSkill\n        &gt;&gt;&gt; skillset = LinearSkillSet(skills=[TransformSkill(), ClassificationSkill(), AnalysisSkill()])\n    \"\"\"\n\n    skill_sequence: List[str] = None\n\n    @model_validator(mode=\"after\")\n    def skill_sequence_validator(self) -&gt; \"LinearSkillSet\":\n        \"\"\"\n        Validates and sets the default order for the skill sequence if not provided.\n\n        Returns:\n            LinearSkillSet: The current instance with updated skill_sequence attribute.\n        \"\"\"\n        if self.skill_sequence is None:\n            # use default skill sequence defined by lexicographical order\n            self.skill_sequence = list(self.skills.keys())\n        if len(self.skill_sequence) != len(self.skills):\n            raise ValueError(\n                f\"skill_sequence must contain all skill names - \"\n                f\"length of skill_sequence is {len(self.skill_sequence)} \"\n                f\"while length of skills is {len(self.skills)}\"\n            )\n        return self\n\n    def apply(\n        self,\n        input: Union[Record, InternalDataFrame],\n        runtime: Runtime,\n        improved_skill: Optional[str] = None,\n    ) -&gt; InternalDataFrame:\n        \"\"\"\n        Sequentially applies each skill on the dataset, enhancing the agent's experience.\n\n        Args:\n            input (InternalDataFrame): Input dataset.\n            runtime (Runtime): The runtime environment in which to apply the skills.\n            improved_skill (Optional[str], optional): Name of the skill to improve. Defaults to None.\n        Returns:\n            InternalDataFrame: Skill predictions.\n        \"\"\"\n        if improved_skill:\n            # start from the specified skill, assuming previous skills have already been applied\n            skill_sequence = self.skill_sequence[\n                self.skill_sequence.index(improved_skill) :\n            ]\n        else:\n            skill_sequence = self.skill_sequence\n        skill_input = input\n        for i, skill_name in enumerate(skill_sequence):\n            skill = self.skills[skill_name]\n            # use input dataset for the first node in the pipeline\n            print_text(f\"Applying skill: {skill_name}\")\n            skill_output = skill.apply(skill_input, runtime)\n            if isinstance(skill, TransformSkill):\n                # Columns to drop from skill_input because they are also in skill_output\n                cols_to_drop = set(skill_output.columns) &amp; set(skill_input.columns)\n                skill_input_reduced = skill_input.drop(columns=cols_to_drop)\n\n                skill_input = skill_input_reduced.merge(\n                    skill_output, left_index=True, right_index=True, how=\"inner\"\n                )\n            elif isinstance(skill, (AnalysisSkill, SynthesisSkill)):\n                skill_input = skill_output\n            else:\n                raise ValueError(f\"Unsupported skill type: {type(skill)}\")\n        if isinstance(skill_input, InternalSeries):\n            skill_input = skill_input.to_frame().T\n        return skill_input\n\n    def __rich__(self):\n        \"\"\"Returns a rich representation of the skill.\"\"\"\n        # TODO: move it to a base class and use repr derived from Skills\n        text = f\"[bold blue]Total Agent Skills: {len(self.skills)}[/bold blue]\\n\\n\"\n        for skill in self.skills.values():\n            text += (\n                f\"[bold underline green]{skill.name}[/bold underline green]\\n\"\n                f\"[green]{skill.instructions}[green]\\n\"\n            )\n        return text\n</code></pre>"},{"location":"skills/#adala.skills.skillset.LinearSkillSet.__rich__","title":"<code>__rich__()</code>","text":"<p>Returns a rich representation of the skill.</p> Source code in <code>adala/skills/skillset.py</code> <pre><code>def __rich__(self):\n    \"\"\"Returns a rich representation of the skill.\"\"\"\n    # TODO: move it to a base class and use repr derived from Skills\n    text = f\"[bold blue]Total Agent Skills: {len(self.skills)}[/bold blue]\\n\\n\"\n    for skill in self.skills.values():\n        text += (\n            f\"[bold underline green]{skill.name}[/bold underline green]\\n\"\n            f\"[green]{skill.instructions}[green]\\n\"\n        )\n    return text\n</code></pre>"},{"location":"skills/#adala.skills.skillset.LinearSkillSet.apply","title":"<code>apply(input, runtime, improved_skill=None)</code>","text":"<p>Sequentially applies each skill on the dataset, enhancing the agent's experience.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>InternalDataFrame</code> <p>Input dataset.</p> required <code>runtime</code> <code>Runtime</code> <p>The runtime environment in which to apply the skills.</p> required <code>improved_skill</code> <code>Optional[str]</code> <p>Name of the skill to improve. Defaults to None.</p> <code>None</code> <p>Returns:     InternalDataFrame: Skill predictions.</p> Source code in <code>adala/skills/skillset.py</code> <pre><code>def apply(\n    self,\n    input: Union[Record, InternalDataFrame],\n    runtime: Runtime,\n    improved_skill: Optional[str] = None,\n) -&gt; InternalDataFrame:\n    \"\"\"\n    Sequentially applies each skill on the dataset, enhancing the agent's experience.\n\n    Args:\n        input (InternalDataFrame): Input dataset.\n        runtime (Runtime): The runtime environment in which to apply the skills.\n        improved_skill (Optional[str], optional): Name of the skill to improve. Defaults to None.\n    Returns:\n        InternalDataFrame: Skill predictions.\n    \"\"\"\n    if improved_skill:\n        # start from the specified skill, assuming previous skills have already been applied\n        skill_sequence = self.skill_sequence[\n            self.skill_sequence.index(improved_skill) :\n        ]\n    else:\n        skill_sequence = self.skill_sequence\n    skill_input = input\n    for i, skill_name in enumerate(skill_sequence):\n        skill = self.skills[skill_name]\n        # use input dataset for the first node in the pipeline\n        print_text(f\"Applying skill: {skill_name}\")\n        skill_output = skill.apply(skill_input, runtime)\n        if isinstance(skill, TransformSkill):\n            # Columns to drop from skill_input because they are also in skill_output\n            cols_to_drop = set(skill_output.columns) &amp; set(skill_input.columns)\n            skill_input_reduced = skill_input.drop(columns=cols_to_drop)\n\n            skill_input = skill_input_reduced.merge(\n                skill_output, left_index=True, right_index=True, how=\"inner\"\n            )\n        elif isinstance(skill, (AnalysisSkill, SynthesisSkill)):\n            skill_input = skill_output\n        else:\n            raise ValueError(f\"Unsupported skill type: {type(skill)}\")\n    if isinstance(skill_input, InternalSeries):\n        skill_input = skill_input.to_frame().T\n    return skill_input\n</code></pre>"},{"location":"skills/#adala.skills.skillset.LinearSkillSet.skill_sequence_validator","title":"<code>skill_sequence_validator()</code>","text":"<p>Validates and sets the default order for the skill sequence if not provided.</p> <p>Returns:</p> Name Type Description <code>LinearSkillSet</code> <code>LinearSkillSet</code> <p>The current instance with updated skill_sequence attribute.</p> Source code in <code>adala/skills/skillset.py</code> <pre><code>@model_validator(mode=\"after\")\ndef skill_sequence_validator(self) -&gt; \"LinearSkillSet\":\n    \"\"\"\n    Validates and sets the default order for the skill sequence if not provided.\n\n    Returns:\n        LinearSkillSet: The current instance with updated skill_sequence attribute.\n    \"\"\"\n    if self.skill_sequence is None:\n        # use default skill sequence defined by lexicographical order\n        self.skill_sequence = list(self.skills.keys())\n    if len(self.skill_sequence) != len(self.skills):\n        raise ValueError(\n            f\"skill_sequence must contain all skill names - \"\n            f\"length of skill_sequence is {len(self.skill_sequence)} \"\n            f\"while length of skills is {len(self.skills)}\"\n        )\n    return self\n</code></pre>"},{"location":"skills/#adala.skills.skillset.ParallelSkillSet","title":"<code>ParallelSkillSet</code>","text":"<p>             Bases: <code>SkillSet</code></p> <p>Represents a set of skills that are acquired simultaneously to reach a goal.</p> <p>In a ParallelSkillSet, each skill can be developed independently of the others. This is useful for agents that require multiple, diverse capabilities, or tasks where each skill contributes a piece of the overall solution.</p> <p>Examples:</p> <p>Create a ParallelSkillSet with a list of skills specified as BaseSkill instances</p> <pre><code>&gt;&gt;&gt; from adala.skills import ParallelSkillSet, ClassificationSkill, TransformSkill\n&gt;&gt;&gt; skillset = ParallelSkillSet(skills=[ClassificationSkill(), TransformSkill()])\n</code></pre> Source code in <code>adala/skills/skillset.py</code> <pre><code>class ParallelSkillSet(SkillSet):\n    \"\"\"\n    Represents a set of skills that are acquired simultaneously to reach a goal.\n\n    In a ParallelSkillSet, each skill can be developed independently of the others. This is useful\n    for agents that require multiple, diverse capabilities, or tasks where each skill contributes a piece of\n    the overall solution.\n\n    Examples:\n        Create a ParallelSkillSet with a list of skills specified as BaseSkill instances\n        &gt;&gt;&gt; from adala.skills import ParallelSkillSet, ClassificationSkill, TransformSkill\n        &gt;&gt;&gt; skillset = ParallelSkillSet(skills=[ClassificationSkill(), TransformSkill()])\n    \"\"\"\n\n    def apply(\n        self,\n        input: Union[InternalSeries, InternalDataFrame],\n        runtime: Runtime,\n        improved_skill: Optional[str] = None,\n    ) -&gt; InternalDataFrame:\n        \"\"\"\n        Applies each skill on the dataset, enhancing the agent's experience.\n\n        Args:\n            input (Union[Record, InternalDataFrame]): Input data\n            runtime (Runtime): The runtime environment in which to apply the skills.\n            improved_skill (Optional[str], optional): Unused in ParallelSkillSet. Defaults to None.\n        Returns:\n            Union[Record, InternalDataFrame]: Skill predictions.\n        \"\"\"\n        if improved_skill:\n            # start from the specified skill, assuming previous skills have already been applied\n            skill_sequence = [improved_skill]\n        else:\n            skill_sequence = list(self.skills.keys())\n\n        skill_outputs = []\n        for i, skill_name in enumerate(skill_sequence):\n            skill = self.skills[skill_name]\n            # use input dataset for the first node in the pipeline\n            print_text(f\"Applying skill: {skill_name}\")\n            skill_output = skill.apply(input, runtime)\n            skill_outputs.append(skill_output)\n        if not skill_outputs:\n            return InternalDataFrame()\n        else:\n            if isinstance(skill_outputs[0], InternalDataFrame):\n                skill_outputs = InternalDataFrameConcat(skill_outputs, axis=1)\n                cols_to_drop = set(input.columns) &amp; set(skill_outputs.columns)\n                skill_input_reduced = input.drop(columns=cols_to_drop)\n\n                return skill_input_reduced.merge(\n                    skill_outputs, left_index=True, right_index=True, how=\"inner\"\n                )\n            elif isinstance(skill_outputs[0], (dict, InternalSeries)):\n                # concatenate output to each row of input\n                output = skill_outputs[0]\n                return InternalDataFrameConcat(\n                    [\n                        input,\n                        InternalDataFrame(\n                            [output] * len(input),\n                            columns=output.index,\n                            index=input.index,\n                        ),\n                    ],\n                    axis=1,\n                )\n            else:\n                raise ValueError(f\"Unsupported output type: {type(skill_outputs[0])}\")\n</code></pre>"},{"location":"skills/#adala.skills.skillset.ParallelSkillSet.apply","title":"<code>apply(input, runtime, improved_skill=None)</code>","text":"<p>Applies each skill on the dataset, enhancing the agent's experience.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Union[Record, InternalDataFrame]</code> <p>Input data</p> required <code>runtime</code> <code>Runtime</code> <p>The runtime environment in which to apply the skills.</p> required <code>improved_skill</code> <code>Optional[str]</code> <p>Unused in ParallelSkillSet. Defaults to None.</p> <code>None</code> <p>Returns:     Union[Record, InternalDataFrame]: Skill predictions.</p> Source code in <code>adala/skills/skillset.py</code> <pre><code>def apply(\n    self,\n    input: Union[InternalSeries, InternalDataFrame],\n    runtime: Runtime,\n    improved_skill: Optional[str] = None,\n) -&gt; InternalDataFrame:\n    \"\"\"\n    Applies each skill on the dataset, enhancing the agent's experience.\n\n    Args:\n        input (Union[Record, InternalDataFrame]): Input data\n        runtime (Runtime): The runtime environment in which to apply the skills.\n        improved_skill (Optional[str], optional): Unused in ParallelSkillSet. Defaults to None.\n    Returns:\n        Union[Record, InternalDataFrame]: Skill predictions.\n    \"\"\"\n    if improved_skill:\n        # start from the specified skill, assuming previous skills have already been applied\n        skill_sequence = [improved_skill]\n    else:\n        skill_sequence = list(self.skills.keys())\n\n    skill_outputs = []\n    for i, skill_name in enumerate(skill_sequence):\n        skill = self.skills[skill_name]\n        # use input dataset for the first node in the pipeline\n        print_text(f\"Applying skill: {skill_name}\")\n        skill_output = skill.apply(input, runtime)\n        skill_outputs.append(skill_output)\n    if not skill_outputs:\n        return InternalDataFrame()\n    else:\n        if isinstance(skill_outputs[0], InternalDataFrame):\n            skill_outputs = InternalDataFrameConcat(skill_outputs, axis=1)\n            cols_to_drop = set(input.columns) &amp; set(skill_outputs.columns)\n            skill_input_reduced = input.drop(columns=cols_to_drop)\n\n            return skill_input_reduced.merge(\n                skill_outputs, left_index=True, right_index=True, how=\"inner\"\n            )\n        elif isinstance(skill_outputs[0], (dict, InternalSeries)):\n            # concatenate output to each row of input\n            output = skill_outputs[0]\n            return InternalDataFrameConcat(\n                [\n                    input,\n                    InternalDataFrame(\n                        [output] * len(input),\n                        columns=output.index,\n                        index=input.index,\n                    ),\n                ],\n                axis=1,\n            )\n        else:\n            raise ValueError(f\"Unsupported output type: {type(skill_outputs[0])}\")\n</code></pre>"},{"location":"skills/#adala.skills.skillset.SkillSet","title":"<code>SkillSet</code>","text":"<p>             Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Represents a collection of interdependent skills aiming to achieve a specific goal.</p> <p>A skill set breaks down the path to achieve a goal into necessary precursor skills. Agents can evolve these skills either in parallel for tasks like self-consistency or sequentially for complex problem decompositions and causal reasoning. In the most generic cases, task decomposition can involve a graph-based approach.</p> <p>Attributes:</p> Name Type Description <code>skills</code> <code>Dict[str, Skill]</code> <p>A dictionary of skills in the skill set.</p> Source code in <code>adala/skills/skillset.py</code> <pre><code>class SkillSet(BaseModel, ABC):\n    \"\"\"\n    Represents a collection of interdependent skills aiming to achieve a specific goal.\n\n    A skill set breaks down the path to achieve a goal into necessary precursor skills.\n    Agents can evolve these skills either in parallel for tasks like self-consistency or\n    sequentially for complex problem decompositions and causal reasoning. In the most generic\n    cases, task decomposition can involve a graph-based approach.\n\n    Attributes:\n        skills (Dict[str, Skill]): A dictionary of skills in the skill set.\n    \"\"\"\n\n    skills: Dict[str, Skill]\n\n    @field_validator(\"skills\", mode=\"before\")\n    def skills_validator(\n        cls, v: Union[List[Skill], Dict[str, Skill]]\n    ) -&gt; Dict[str, Skill]:\n        \"\"\"\n        Validates and converts the skills attribute to a dictionary of skill names to BaseSkill instances.\n\n        Args:\n            v (Union[List[Skill], Dict[str, Skill]]): The skills attribute to validate and convert.\n\n        Returns:\n            Dict[str, BaseSkill]: Dictionary mapping skill names to their corresponding BaseSkill instances.\n        \"\"\"\n        skills = OrderedDict()\n        if not v:\n            return skills\n\n        elif isinstance(v, list) and isinstance(v[0], Skill):\n            # convert list of skill names to dictionary\n            for skill in v:\n                skills[skill.name] = skill\n        elif isinstance(v, dict):\n            skills = v\n        else:\n            raise ValueError(f\"skills must be a list or dictionary, not {type(skills)}\")\n        return skills\n\n    @abstractmethod\n    def apply(\n        self,\n        input: Union[Record, InternalDataFrame],\n        runtime: Runtime,\n        improved_skill: Optional[str] = None,\n    ) -&gt; InternalDataFrame:\n        \"\"\"\n        Apply the skill set to a dataset using a specified runtime.\n\n        Args:\n            input (Union[Record, InternalDataFrame]): Input data to apply the skill set to.\n            runtime (Runtime): The runtime environment in which to apply the skills.\n            improved_skill (Optional[str], optional): Name of the skill to start from (to optimize calculations). Defaults to None.\n        Returns:\n            InternalDataFrame: Skill predictions.\n        \"\"\"\n\n    def __getitem__(self, skill_name) -&gt; Skill:\n        \"\"\"\n        Select skill by name.\n\n        Args:\n            skill_name (str): Name of the skill to select.\n\n        Returns:\n            BaseSkill: Skill\n        \"\"\"\n        return self.skills[skill_name]\n\n    def __setitem__(self, skill_name, skill: Skill):\n        \"\"\"\n        Set skill by name.\n\n        Args:\n            skill_name (str): Name of the skill to set.\n            skill (BaseSkill): Skill to set.\n        \"\"\"\n        self.skills[skill_name] = skill\n\n    def get_skill_names(self) -&gt; List[str]:\n        \"\"\"\n        Get list of skill names.\n\n        Returns:\n            List[str]: List of skill names.\n        \"\"\"\n        return list(self.skills.keys())\n\n    def get_skill_outputs(self) -&gt; Dict[str, str]:\n        \"\"\"\n        Get dictionary of skill outputs.\n\n        Returns:\n            Dict[str, str]: Dictionary of skill outputs. Keys are output names and values are skill names\n        \"\"\"\n        return {\n            field: skill.name\n            for skill in self.skills.values()\n            for field in skill.get_output_fields()\n        }\n</code></pre>"},{"location":"skills/#adala.skills.skillset.SkillSet.__getitem__","title":"<code>__getitem__(skill_name)</code>","text":"<p>Select skill by name.</p> <p>Parameters:</p> Name Type Description Default <code>skill_name</code> <code>str</code> <p>Name of the skill to select.</p> required <p>Returns:</p> Name Type Description <code>BaseSkill</code> <code>Skill</code> <p>Skill</p> Source code in <code>adala/skills/skillset.py</code> <pre><code>def __getitem__(self, skill_name) -&gt; Skill:\n    \"\"\"\n    Select skill by name.\n\n    Args:\n        skill_name (str): Name of the skill to select.\n\n    Returns:\n        BaseSkill: Skill\n    \"\"\"\n    return self.skills[skill_name]\n</code></pre>"},{"location":"skills/#adala.skills.skillset.SkillSet.__setitem__","title":"<code>__setitem__(skill_name, skill)</code>","text":"<p>Set skill by name.</p> <p>Parameters:</p> Name Type Description Default <code>skill_name</code> <code>str</code> <p>Name of the skill to set.</p> required <code>skill</code> <code>BaseSkill</code> <p>Skill to set.</p> required Source code in <code>adala/skills/skillset.py</code> <pre><code>def __setitem__(self, skill_name, skill: Skill):\n    \"\"\"\n    Set skill by name.\n\n    Args:\n        skill_name (str): Name of the skill to set.\n        skill (BaseSkill): Skill to set.\n    \"\"\"\n    self.skills[skill_name] = skill\n</code></pre>"},{"location":"skills/#adala.skills.skillset.SkillSet.apply","title":"<code>apply(input, runtime, improved_skill=None)</code>  <code>abstractmethod</code>","text":"<p>Apply the skill set to a dataset using a specified runtime.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Union[Record, InternalDataFrame]</code> <p>Input data to apply the skill set to.</p> required <code>runtime</code> <code>Runtime</code> <p>The runtime environment in which to apply the skills.</p> required <code>improved_skill</code> <code>Optional[str]</code> <p>Name of the skill to start from (to optimize calculations). Defaults to None.</p> <code>None</code> <p>Returns:     InternalDataFrame: Skill predictions.</p> Source code in <code>adala/skills/skillset.py</code> <pre><code>@abstractmethod\ndef apply(\n    self,\n    input: Union[Record, InternalDataFrame],\n    runtime: Runtime,\n    improved_skill: Optional[str] = None,\n) -&gt; InternalDataFrame:\n    \"\"\"\n    Apply the skill set to a dataset using a specified runtime.\n\n    Args:\n        input (Union[Record, InternalDataFrame]): Input data to apply the skill set to.\n        runtime (Runtime): The runtime environment in which to apply the skills.\n        improved_skill (Optional[str], optional): Name of the skill to start from (to optimize calculations). Defaults to None.\n    Returns:\n        InternalDataFrame: Skill predictions.\n    \"\"\"\n</code></pre>"},{"location":"skills/#adala.skills.skillset.SkillSet.get_skill_names","title":"<code>get_skill_names()</code>","text":"<p>Get list of skill names.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of skill names.</p> Source code in <code>adala/skills/skillset.py</code> <pre><code>def get_skill_names(self) -&gt; List[str]:\n    \"\"\"\n    Get list of skill names.\n\n    Returns:\n        List[str]: List of skill names.\n    \"\"\"\n    return list(self.skills.keys())\n</code></pre>"},{"location":"skills/#adala.skills.skillset.SkillSet.get_skill_outputs","title":"<code>get_skill_outputs()</code>","text":"<p>Get dictionary of skill outputs.</p> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dict[str, str]: Dictionary of skill outputs. Keys are output names and values are skill names</p> Source code in <code>adala/skills/skillset.py</code> <pre><code>def get_skill_outputs(self) -&gt; Dict[str, str]:\n    \"\"\"\n    Get dictionary of skill outputs.\n\n    Returns:\n        Dict[str, str]: Dictionary of skill outputs. Keys are output names and values are skill names\n    \"\"\"\n    return {\n        field: skill.name\n        for skill in self.skills.values()\n        for field in skill.get_output_fields()\n    }\n</code></pre>"},{"location":"skills/#adala.skills.skillset.SkillSet.skills_validator","title":"<code>skills_validator(v)</code>","text":"<p>Validates and converts the skills attribute to a dictionary of skill names to BaseSkill instances.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>Union[List[Skill], Dict[str, Skill]]</code> <p>The skills attribute to validate and convert.</p> required <p>Returns:</p> Type Description <code>Dict[str, Skill]</code> <p>Dict[str, BaseSkill]: Dictionary mapping skill names to their corresponding BaseSkill instances.</p> Source code in <code>adala/skills/skillset.py</code> <pre><code>@field_validator(\"skills\", mode=\"before\")\ndef skills_validator(\n    cls, v: Union[List[Skill], Dict[str, Skill]]\n) -&gt; Dict[str, Skill]:\n    \"\"\"\n    Validates and converts the skills attribute to a dictionary of skill names to BaseSkill instances.\n\n    Args:\n        v (Union[List[Skill], Dict[str, Skill]]): The skills attribute to validate and convert.\n\n    Returns:\n        Dict[str, BaseSkill]: Dictionary mapping skill names to their corresponding BaseSkill instances.\n    \"\"\"\n    skills = OrderedDict()\n    if not v:\n        return skills\n\n    elif isinstance(v, list) and isinstance(v[0], Skill):\n        # convert list of skill names to dictionary\n        for skill in v:\n            skills[skill.name] = skill\n    elif isinstance(v, dict):\n        skills = v\n    else:\n        raise ValueError(f\"skills must be a list or dictionary, not {type(skills)}\")\n    return skills\n</code></pre>"},{"location":"utils/","title":"Utils","text":""},{"location":"utils/#adala.utils.internal_data.InternalDataFrameConcat","title":"<code>InternalDataFrameConcat(dfs, **kwargs)</code>","text":"<p>Concatenate dataframes.</p> <p>Parameters:</p> Name Type Description Default <code>dfs</code> <code>Iterable[InternalDataFrame]</code> <p>The dataframes to concatenate.</p> required <p>Returns:</p> Name Type Description <code>InternalDataFrame</code> <code>InternalDataFrame</code> <p>The concatenated dataframe.</p> Source code in <code>adala/utils/internal_data.py</code> <pre><code>def InternalDataFrameConcat(\n    dfs: Iterable[InternalDataFrame], **kwargs\n) -&gt; InternalDataFrame:\n    \"\"\"\n    Concatenate dataframes.\n\n    Args:\n        dfs (Iterable[InternalDataFrame]): The dataframes to concatenate.\n\n    Returns:\n        InternalDataFrame: The concatenated dataframe.\n    \"\"\"\n    return pd.concat(dfs, **kwargs)\n</code></pre>"},{"location":"utils/#adala.utils.logs.print_dataframe","title":"<code>print_dataframe(dataframe)</code>","text":"<p>Print dataframe to console.</p> Source code in <code>adala/utils/logs.py</code> <pre><code>def print_dataframe(dataframe: InternalDataFrame):\n    \"\"\"\n    Print dataframe to console.\n    \"\"\"\n    num_rows = 5\n    table = Table(show_header=True, header_style=\"bold magenta\")\n    # index_name = dataframe.index.name or 'index'\n    # table.add_column(index_name)\n\n    for column in dataframe.columns:\n        table.add_column(str(column))\n\n    for index, value_list in enumerate(dataframe.iloc[:num_rows].values.tolist()):\n        # row = [str(index)]\n        row = []\n        row += [str(x) for x in value_list]\n        table.add_row(*row)\n\n    # Update the style of the table\n    table.row_styles = [\"none\", \"dim\"]\n    table.box = box.SIMPLE_HEAD\n\n    console.print(table)\n</code></pre>"},{"location":"utils/#adala.utils.logs.print_error","title":"<code>print_error(text)</code>","text":"<p>Print error message to console.</p> Source code in <code>adala/utils/logs.py</code> <pre><code>def print_error(text: str):\n    \"\"\"\n    Print error message to console.\n    \"\"\"\n    error_console.print(text)\n</code></pre>"},{"location":"utils/#adala.utils.logs.print_series","title":"<code>print_series(data)</code>","text":"<p>Print series to console.</p> Source code in <code>adala/utils/logs.py</code> <pre><code>def print_series(data: InternalSeries):\n    \"\"\"\n    Print series to console.\n    \"\"\"\n\n    # Create a Rich Table with a column for each series value\n    table = Table(show_header=True, header_style=\"bold magenta\")\n\n    # Add a column for each value in the series with the index as the header\n    for index in data.index:\n        table.add_column(str(index))\n\n    # Add a single row with all the values from the series\n    table.add_row(*[str(value) for value in data])\n\n    # Print the table with the Rich console\n    console.print(table)\n</code></pre>"},{"location":"utils/#adala.utils.logs.print_text","title":"<code>print_text(text, style=None, streaming_style=False)</code>","text":"<p>Print text to console with optional style and streaming style.</p> Source code in <code>adala/utils/logs.py</code> <pre><code>def print_text(text: str, style=None, streaming_style=False):\n    \"\"\"\n    Print text to console with optional style and streaming style.\n    \"\"\"\n    if streaming_style:\n        for char in text:\n            console.print(char, sep=\"\", end=\"\", style=style)\n            time.sleep(0.01)\n        console.print()\n    else:\n        console.print(text, style=style)\n</code></pre>"},{"location":"utils/#adala.utils.matching.fuzzy_match","title":"<code>fuzzy_match(x, y, threshold=0.8)</code>","text":"<p>Fuzzy match string values in two series.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>InternalSeries</code> <p>The first series.</p> required <code>y</code> <code>InternalSeries</code> <p>The second series.</p> required <code>threshold</code> <code>float</code> <p>The threshold to use for fuzzy matching. Defaults to 0.8.</p> <code>0.8</code> <p>Returns:</p> Name Type Description <code>InternalSeries</code> <code>InternalSeries</code> <p>The series with fuzzy match results.</p> Source code in <code>adala/utils/matching.py</code> <pre><code>def fuzzy_match(x: InternalSeries, y: InternalSeries, threshold=0.8) -&gt; InternalSeries:\n    \"\"\"\n    Fuzzy match string values in two series.\n\n    Args:\n        x (InternalSeries): The first series.\n        y (InternalSeries): The second series.\n        threshold (float): The threshold to use for fuzzy matching. Defaults to 0.8.\n\n    Returns:\n        InternalSeries: The series with fuzzy match results.\n    \"\"\"\n    result = x.combine(y, lambda x, y: _fuzzy_match(x, y, threshold))\n    return result\n</code></pre>"}]}